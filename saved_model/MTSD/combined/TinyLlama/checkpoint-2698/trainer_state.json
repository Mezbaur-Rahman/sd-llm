{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 2698,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03,
      "grad_norm": 1.3075611591339111,
      "learning_rate": 0.00019999891370152373,
      "loss": 3.143,
      "step": 10
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.6303632259368896,
      "learning_rate": 0.00019999565482969577,
      "loss": 2.3399,
      "step": 20
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.3852665424346924,
      "learning_rate": 0.00019999022345531834,
      "loss": 1.7321,
      "step": 30
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.7239919900894165,
      "learning_rate": 0.00019998261969639324,
      "loss": 1.3512,
      "step": 40
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.5635300278663635,
      "learning_rate": 0.00019997284371811954,
      "loss": 1.3388,
      "step": 50
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.6905417442321777,
      "learning_rate": 0.00019996089573288983,
      "loss": 1.2353,
      "step": 60
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.5719131827354431,
      "learning_rate": 0.00019994677600028568,
      "loss": 1.204,
      "step": 70
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.7672732472419739,
      "learning_rate": 0.000199930484827072,
      "loss": 1.2053,
      "step": 80
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.5732651948928833,
      "learning_rate": 0.0001999120225671903,
      "loss": 1.1687,
      "step": 90
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5168470740318298,
      "learning_rate": 0.00019989138962175104,
      "loss": 1.2147,
      "step": 100
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6930299401283264,
      "learning_rate": 0.00019986858643902502,
      "loss": 1.1572,
      "step": 110
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.49699515104293823,
      "learning_rate": 0.00019984361351443343,
      "loss": 1.1542,
      "step": 120
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.5159195065498352,
      "learning_rate": 0.00019981647139053738,
      "loss": 1.144,
      "step": 130
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.4984678328037262,
      "learning_rate": 0.00019978716065702568,
      "loss": 1.1546,
      "step": 140
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.4763847887516022,
      "learning_rate": 0.00019975568195070253,
      "loss": 1.1143,
      "step": 150
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.5145814418792725,
      "learning_rate": 0.00019972203595547335,
      "loss": 1.107,
      "step": 160
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.5763629674911499,
      "learning_rate": 0.00019968622340232995,
      "loss": 1.1037,
      "step": 170
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.5307971835136414,
      "learning_rate": 0.0001996482450693348,
      "loss": 1.1021,
      "step": 180
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4900030493736267,
      "learning_rate": 0.000199608101781604,
      "loss": 1.1181,
      "step": 190
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.44996020197868347,
      "learning_rate": 0.00019956579441128943,
      "loss": 1.1382,
      "step": 200
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.5349639654159546,
      "learning_rate": 0.00019952132387755965,
      "loss": 1.1026,
      "step": 210
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.46941131353378296,
      "learning_rate": 0.00019947469114658017,
      "loss": 1.1339,
      "step": 220
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.6132367849349976,
      "learning_rate": 0.00019942589723149232,
      "loss": 1.1629,
      "step": 230
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.5833242535591125,
      "learning_rate": 0.00019937494319239113,
      "loss": 1.1326,
      "step": 240
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.4129420220851898,
      "learning_rate": 0.00019932183013630255,
      "loss": 1.1276,
      "step": 250
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.5098044872283936,
      "learning_rate": 0.00019926655921715923,
      "loss": 1.1047,
      "step": 260
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.48354601860046387,
      "learning_rate": 0.0001992091316357754,
      "loss": 1.1527,
      "step": 270
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.4980199337005615,
      "learning_rate": 0.00019914954863982107,
      "loss": 1.0593,
      "step": 280
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.4511754512786865,
      "learning_rate": 0.0001990878115237945,
      "loss": 1.1236,
      "step": 290
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.5281345248222351,
      "learning_rate": 0.0001990239216289944,
      "loss": 1.1307,
      "step": 300
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.49406754970550537,
      "learning_rate": 0.0001989578803434907,
      "loss": 1.1253,
      "step": 310
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.40307730436325073,
      "learning_rate": 0.00019888968910209434,
      "loss": 1.1073,
      "step": 320
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.4692971110343933,
      "learning_rate": 0.00019881934938632614,
      "loss": 1.0946,
      "step": 330
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.49514269828796387,
      "learning_rate": 0.00019874686272438466,
      "loss": 1.1216,
      "step": 340
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.4385001063346863,
      "learning_rate": 0.00019867223069111288,
      "loss": 1.0631,
      "step": 350
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.5237520933151245,
      "learning_rate": 0.00019859545490796411,
      "loss": 1.1025,
      "step": 360
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.45556092262268066,
      "learning_rate": 0.00019851653704296664,
      "loss": 1.1368,
      "step": 370
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.4565790295600891,
      "learning_rate": 0.00019843547881068764,
      "loss": 1.1072,
      "step": 380
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4629579186439514,
      "learning_rate": 0.00019835228197219573,
      "loss": 1.0693,
      "step": 390
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.484601229429245,
      "learning_rate": 0.00019826694833502295,
      "loss": 1.0472,
      "step": 400
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.5221013426780701,
      "learning_rate": 0.00019817947975312526,
      "loss": 1.0947,
      "step": 410
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.47265195846557617,
      "learning_rate": 0.00019808987812684246,
      "loss": 1.1088,
      "step": 420
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.48853638768196106,
      "learning_rate": 0.00019799814540285668,
      "loss": 1.0581,
      "step": 430
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.45954108238220215,
      "learning_rate": 0.00019790428357415032,
      "loss": 1.1373,
      "step": 440
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.4909793436527252,
      "learning_rate": 0.00019780829467996262,
      "loss": 1.0918,
      "step": 450
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.4872545599937439,
      "learning_rate": 0.00019771018080574535,
      "loss": 1.1121,
      "step": 460
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.48949578404426575,
      "learning_rate": 0.00019760994408311757,
      "loss": 1.1256,
      "step": 470
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.39528682827949524,
      "learning_rate": 0.00019750758668981924,
      "loss": 1.1134,
      "step": 480
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.5441740155220032,
      "learning_rate": 0.000197403110849664,
      "loss": 1.0463,
      "step": 490
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.48039916157722473,
      "learning_rate": 0.00019729651883249074,
      "loss": 1.1238,
      "step": 500
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.5000181794166565,
      "learning_rate": 0.00019718781295411438,
      "loss": 1.0879,
      "step": 510
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.47498947381973267,
      "learning_rate": 0.00019707699557627555,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.4218917787075043,
      "learning_rate": 0.00019696406910658918,
      "loss": 1.0946,
      "step": 530
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5329504609107971,
      "learning_rate": 0.00019684903599849233,
      "loss": 1.1378,
      "step": 540
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.5263418555259705,
      "learning_rate": 0.0001967318987511908,
      "loss": 1.0256,
      "step": 550
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.4427650272846222,
      "learning_rate": 0.00019661265990960483,
      "loss": 1.0445,
      "step": 560
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.4551290273666382,
      "learning_rate": 0.00019649132206431394,
      "loss": 1.1043,
      "step": 570
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.593726396560669,
      "learning_rate": 0.00019636788785150038,
      "loss": 1.0986,
      "step": 580
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.41312745213508606,
      "learning_rate": 0.0001962423599528921,
      "loss": 1.0529,
      "step": 590
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.46348801255226135,
      "learning_rate": 0.00019611474109570444,
      "loss": 1.0872,
      "step": 600
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.47716087102890015,
      "learning_rate": 0.00019598503405258077,
      "loss": 1.0558,
      "step": 610
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.4998435080051422,
      "learning_rate": 0.00019585324164153237,
      "loss": 1.0639,
      "step": 620
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.41655024886131287,
      "learning_rate": 0.00019571936672587716,
      "loss": 1.0767,
      "step": 630
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.41480180621147156,
      "learning_rate": 0.00019558341221417744,
      "loss": 1.1144,
      "step": 640
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.42585453391075134,
      "learning_rate": 0.0001954453810601768,
      "loss": 1.0908,
      "step": 650
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.4502318799495697,
      "learning_rate": 0.00019530527626273591,
      "loss": 1.111,
      "step": 660
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.39226168394088745,
      "learning_rate": 0.00019516310086576733,
      "loss": 1.0432,
      "step": 670
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.5012441277503967,
      "learning_rate": 0.00019501885795816936,
      "loss": 1.0818,
      "step": 680
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.44976887106895447,
      "learning_rate": 0.00019487255067375907,
      "loss": 1.008,
      "step": 690
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.42139679193496704,
      "learning_rate": 0.000194724182191204,
      "loss": 1.0488,
      "step": 700
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.5623383522033691,
      "learning_rate": 0.00019457375573395333,
      "loss": 1.0838,
      "step": 710
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.40040522813796997,
      "learning_rate": 0.00019442127457016767,
      "loss": 1.1068,
      "step": 720
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.4295685291290283,
      "learning_rate": 0.00019426674201264814,
      "loss": 1.0237,
      "step": 730
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.4583430290222168,
      "learning_rate": 0.00019411016141876437,
      "loss": 1.0699,
      "step": 740
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.5310168862342834,
      "learning_rate": 0.00019395153619038158,
      "loss": 1.0261,
      "step": 750
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.4459216892719269,
      "learning_rate": 0.00019379086977378663,
      "loss": 1.0722,
      "step": 760
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.442926824092865,
      "learning_rate": 0.00019362816565961322,
      "loss": 1.0408,
      "step": 770
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.4096292555332184,
      "learning_rate": 0.00019346342738276592,
      "loss": 1.0738,
      "step": 780
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.5359726548194885,
      "learning_rate": 0.0001932966585223436,
      "loss": 1.0907,
      "step": 790
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.43491387367248535,
      "learning_rate": 0.00019312786270156136,
      "loss": 1.0343,
      "step": 800
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.5080567598342896,
      "learning_rate": 0.0001929570435876721,
      "loss": 1.0434,
      "step": 810
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.4781583547592163,
      "learning_rate": 0.00019278420489188668,
      "loss": 1.0415,
      "step": 820
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.39566144347190857,
      "learning_rate": 0.0001926093503692933,
      "loss": 1.0718,
      "step": 830
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.46392521262168884,
      "learning_rate": 0.00019243248381877606,
      "loss": 1.073,
      "step": 840
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.5258653163909912,
      "learning_rate": 0.00019225360908293218,
      "loss": 1.0335,
      "step": 850
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.4365910291671753,
      "learning_rate": 0.00019207273004798872,
      "loss": 1.0429,
      "step": 860
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.43422266840934753,
      "learning_rate": 0.00019188985064371816,
      "loss": 1.0758,
      "step": 870
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.4220462739467621,
      "learning_rate": 0.00019170497484335278,
      "loss": 1.0639,
      "step": 880
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.40976470708847046,
      "learning_rate": 0.0001915181066634986,
      "loss": 1.0469,
      "step": 890
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.4362642467021942,
      "learning_rate": 0.00019132925016404804,
      "loss": 1.0293,
      "step": 900
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.46935397386550903,
      "learning_rate": 0.0001911384094480916,
      "loss": 1.0348,
      "step": 910
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.4438968598842621,
      "learning_rate": 0.00019094558866182893,
      "loss": 1.0386,
      "step": 920
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.471836119890213,
      "learning_rate": 0.00019075079199447846,
      "loss": 1.0528,
      "step": 930
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.40119388699531555,
      "learning_rate": 0.00019055402367818672,
      "loss": 1.0698,
      "step": 940
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.47413596510887146,
      "learning_rate": 0.0001903552879879362,
      "loss": 0.9962,
      "step": 950
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.5058668851852417,
      "learning_rate": 0.00019015458924145228,
      "loss": 1.062,
      "step": 960
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.42623311281204224,
      "learning_rate": 0.00018995193179910998,
      "loss": 1.0893,
      "step": 970
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.3833693563938141,
      "learning_rate": 0.00018974732006383862,
      "loss": 1.093,
      "step": 980
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.4372406303882599,
      "learning_rate": 0.00018954075848102658,
      "loss": 1.082,
      "step": 990
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.4586900472640991,
      "learning_rate": 0.00018933225153842446,
      "loss": 1.0778,
      "step": 1000
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.43793752789497375,
      "learning_rate": 0.00018912180376604777,
      "loss": 1.08,
      "step": 1010
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.3763284683227539,
      "learning_rate": 0.00018890941973607843,
      "loss": 1.0367,
      "step": 1020
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.4240141212940216,
      "learning_rate": 0.00018869510406276536,
      "loss": 1.0402,
      "step": 1030
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.40305736660957336,
      "learning_rate": 0.00018847886140232437,
      "loss": 1.0191,
      "step": 1040
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.4922249913215637,
      "learning_rate": 0.00018826069645283688,
      "loss": 1.0051,
      "step": 1050
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.4305824041366577,
      "learning_rate": 0.00018804061395414796,
      "loss": 1.0107,
      "step": 1060
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.4487874507904053,
      "learning_rate": 0.00018781861868776326,
      "loss": 1.0711,
      "step": 1070
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.4312799274921417,
      "learning_rate": 0.0001875947154767452,
      "loss": 1.0328,
      "step": 1080
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.39055824279785156,
      "learning_rate": 0.00018736890918560808,
      "loss": 1.0223,
      "step": 1090
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.469806432723999,
      "learning_rate": 0.0001871412047202125,
      "loss": 1.021,
      "step": 1100
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.39434587955474854,
      "learning_rate": 0.00018691160702765877,
      "loss": 1.0415,
      "step": 1110
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.42390188574790955,
      "learning_rate": 0.00018668012109617933,
      "loss": 1.0831,
      "step": 1120
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.4508221745491028,
      "learning_rate": 0.0001864467519550305,
      "loss": 1.0665,
      "step": 1130
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.456382155418396,
      "learning_rate": 0.00018621150467438308,
      "loss": 1.0191,
      "step": 1140
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.4391513764858246,
      "learning_rate": 0.0001859743843652124,
      "loss": 1.0308,
      "step": 1150
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.45625409483909607,
      "learning_rate": 0.000185735396179187,
      "loss": 1.0133,
      "step": 1160
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.4622664153575897,
      "learning_rate": 0.00018549454530855696,
      "loss": 1.036,
      "step": 1170
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.4713175296783447,
      "learning_rate": 0.00018525183698604096,
      "loss": 1.0402,
      "step": 1180
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.4186880886554718,
      "learning_rate": 0.0001850072764847126,
      "loss": 1.07,
      "step": 1190
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.4677225351333618,
      "learning_rate": 0.00018476086911788585,
      "loss": 1.0476,
      "step": 1200
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.4261798858642578,
      "learning_rate": 0.00018451262023899974,
      "loss": 1.0704,
      "step": 1210
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.4047650694847107,
      "learning_rate": 0.00018426253524150176,
      "loss": 1.0369,
      "step": 1220
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.45297375321388245,
      "learning_rate": 0.000184010619558731,
      "loss": 0.957,
      "step": 1230
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.4475320875644684,
      "learning_rate": 0.0001837568786637999,
      "loss": 1.0196,
      "step": 1240
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.4451870918273926,
      "learning_rate": 0.00018350131806947536,
      "loss": 1.0314,
      "step": 1250
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.4011303782463074,
      "learning_rate": 0.00018324394332805912,
      "loss": 1.0676,
      "step": 1260
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.4362086355686188,
      "learning_rate": 0.00018298476003126695,
      "loss": 1.0406,
      "step": 1270
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.4243050515651703,
      "learning_rate": 0.00018272377381010725,
      "loss": 1.0624,
      "step": 1280
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.4446929097175598,
      "learning_rate": 0.0001824609903347587,
      "loss": 1.0577,
      "step": 1290
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.4145580530166626,
      "learning_rate": 0.00018219641531444714,
      "loss": 1.0161,
      "step": 1300
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.48749443888664246,
      "learning_rate": 0.00018193005449732136,
      "loss": 1.042,
      "step": 1310
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.4403590261936188,
      "learning_rate": 0.0001816619136703283,
      "loss": 1.0185,
      "step": 1320
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.43967655301094055,
      "learning_rate": 0.00018139199865908743,
      "loss": 1.0225,
      "step": 1330
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.418138712644577,
      "learning_rate": 0.0001811203153277641,
      "loss": 1.033,
      "step": 1340
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.5155399441719055,
      "learning_rate": 0.00018084686957894205,
      "loss": 1.0607,
      "step": 1350
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.49232617020606995,
      "learning_rate": 0.0001805716673534953,
      "loss": 1.0296,
      "step": 1360
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.46275854110717773,
      "learning_rate": 0.000180294714630459,
      "loss": 1.0483,
      "step": 1370
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.42547962069511414,
      "learning_rate": 0.0001800160174268996,
      "loss": 1.0051,
      "step": 1380
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.41392627358436584,
      "learning_rate": 0.00017973558179778403,
      "loss": 1.0127,
      "step": 1390
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.48813772201538086,
      "learning_rate": 0.00017945341383584818,
      "loss": 1.0782,
      "step": 1400
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.45069167017936707,
      "learning_rate": 0.00017916951967146468,
      "loss": 1.0434,
      "step": 1410
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.4970340430736542,
      "learning_rate": 0.0001788839054725094,
      "loss": 0.9948,
      "step": 1420
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.4905558228492737,
      "learning_rate": 0.0001785965774442278,
      "loss": 1.0339,
      "step": 1430
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.4741889238357544,
      "learning_rate": 0.00017830754182909984,
      "loss": 1.0432,
      "step": 1440
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.4571399986743927,
      "learning_rate": 0.0001780168049067045,
      "loss": 1.0004,
      "step": 1450
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.4189036190509796,
      "learning_rate": 0.0001777243729935832,
      "loss": 1.0058,
      "step": 1460
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.4529610574245453,
      "learning_rate": 0.00017743025244310295,
      "loss": 1.0526,
      "step": 1470
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.44099265336990356,
      "learning_rate": 0.0001771344496453177,
      "loss": 1.0524,
      "step": 1480
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.40829530358314514,
      "learning_rate": 0.0001768369710268301,
      "loss": 0.9478,
      "step": 1490
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.48698168992996216,
      "learning_rate": 0.00017653782305065158,
      "loss": 1.0004,
      "step": 1500
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.44064176082611084,
      "learning_rate": 0.0001762370122160619,
      "loss": 1.0388,
      "step": 1510
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.4465246796607971,
      "learning_rate": 0.00017593454505846805,
      "loss": 0.9899,
      "step": 1520
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.5074998736381531,
      "learning_rate": 0.00017563042814926235,
      "loss": 1.0388,
      "step": 1530
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.4274372160434723,
      "learning_rate": 0.00017532466809567948,
      "loss": 0.9867,
      "step": 1540
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.4556327760219574,
      "learning_rate": 0.00017501727154065305,
      "loss": 1.0052,
      "step": 1550
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.4467003047466278,
      "learning_rate": 0.00017470824516267123,
      "loss": 0.9731,
      "step": 1560
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.5294767022132874,
      "learning_rate": 0.0001743975956756317,
      "loss": 1.0094,
      "step": 1570
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.5173203349113464,
      "learning_rate": 0.00017408532982869575,
      "loss": 1.0172,
      "step": 1580
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.4737870395183563,
      "learning_rate": 0.00017377145440614165,
      "loss": 0.9882,
      "step": 1590
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.42433103919029236,
      "learning_rate": 0.00017345597622721727,
      "loss": 1.0487,
      "step": 1600
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.4253719449043274,
      "learning_rate": 0.00017313890214599192,
      "loss": 1.0523,
      "step": 1610
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.42855310440063477,
      "learning_rate": 0.00017282023905120742,
      "loss": 0.9749,
      "step": 1620
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.4370183050632477,
      "learning_rate": 0.00017249999386612842,
      "loss": 1.0354,
      "step": 1630
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.4149131774902344,
      "learning_rate": 0.00017217817354839212,
      "loss": 0.9778,
      "step": 1640
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.39503368735313416,
      "learning_rate": 0.00017185478508985688,
      "loss": 1.0103,
      "step": 1650
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.42728662490844727,
      "learning_rate": 0.00017152983551645053,
      "loss": 1.0222,
      "step": 1660
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.4645211398601532,
      "learning_rate": 0.00017120333188801756,
      "loss": 1.0353,
      "step": 1670
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.44063910841941833,
      "learning_rate": 0.0001708752812981659,
      "loss": 1.0215,
      "step": 1680
    },
    {
      "epoch": 5.01,
      "grad_norm": 0.43445390462875366,
      "learning_rate": 0.00017054569087411264,
      "loss": 1.0503,
      "step": 1690
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.41584348678588867,
      "learning_rate": 0.00017021456777652926,
      "loss": 1.043,
      "step": 1700
    },
    {
      "epoch": 5.07,
      "grad_norm": 0.4499960243701935,
      "learning_rate": 0.00016988191919938616,
      "loss": 0.9378,
      "step": 1710
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.5038823485374451,
      "learning_rate": 0.00016954775236979614,
      "loss": 0.9954,
      "step": 1720
    },
    {
      "epoch": 5.13,
      "grad_norm": 0.4295519292354584,
      "learning_rate": 0.00016921207454785754,
      "loss": 1.0435,
      "step": 1730
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.46564555168151855,
      "learning_rate": 0.00016887489302649654,
      "loss": 0.993,
      "step": 1740
    },
    {
      "epoch": 5.19,
      "grad_norm": 0.4542476236820221,
      "learning_rate": 0.00016853621513130857,
      "loss": 0.9717,
      "step": 1750
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.42425063252449036,
      "learning_rate": 0.00016819604822039926,
      "loss": 0.9972,
      "step": 1760
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.43179255723953247,
      "learning_rate": 0.00016785439968422457,
      "loss": 1.0363,
      "step": 1770
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.47163426876068115,
      "learning_rate": 0.00016751127694543012,
      "loss": 1.0022,
      "step": 1780
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.46362826228141785,
      "learning_rate": 0.00016716668745869019,
      "loss": 1.0038,
      "step": 1790
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.4425969123840332,
      "learning_rate": 0.00016682063871054534,
      "loss": 0.9881,
      "step": 1800
    },
    {
      "epoch": 5.37,
      "grad_norm": 0.39778101444244385,
      "learning_rate": 0.00016647313821924022,
      "loss": 1.0488,
      "step": 1810
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.4620179533958435,
      "learning_rate": 0.00016612419353455988,
      "loss": 1.0063,
      "step": 1820
    },
    {
      "epoch": 5.43,
      "grad_norm": 0.46278634667396545,
      "learning_rate": 0.0001657738122376659,
      "loss": 1.0213,
      "step": 1830
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.4129514694213867,
      "learning_rate": 0.00016542200194093168,
      "loss": 0.9897,
      "step": 1840
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.4359697103500366,
      "learning_rate": 0.000165068770287777,
      "loss": 1.0218,
      "step": 1850
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.44216758012771606,
      "learning_rate": 0.00016471412495250195,
      "loss": 0.942,
      "step": 1860
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.4601258635520935,
      "learning_rate": 0.00016435807364012034,
      "loss": 0.9548,
      "step": 1870
    },
    {
      "epoch": 5.57,
      "grad_norm": 0.4706811010837555,
      "learning_rate": 0.00016400062408619207,
      "loss": 0.9416,
      "step": 1880
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.4820459187030792,
      "learning_rate": 0.00016364178405665534,
      "loss": 0.9829,
      "step": 1890
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.4250728189945221,
      "learning_rate": 0.00016328156134765763,
      "loss": 0.9873,
      "step": 1900
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.42578601837158203,
      "learning_rate": 0.00016291996378538652,
      "loss": 1.0081,
      "step": 1910
    },
    {
      "epoch": 5.69,
      "grad_norm": 0.46262380480766296,
      "learning_rate": 0.00016255699922589968,
      "loss": 0.9972,
      "step": 1920
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.5049532055854797,
      "learning_rate": 0.00016219267555495407,
      "loss": 1.023,
      "step": 1930
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.46424850821495056,
      "learning_rate": 0.00016182700068783463,
      "loss": 0.9801,
      "step": 1940
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.5047088265419006,
      "learning_rate": 0.00016145998256918238,
      "loss": 0.978,
      "step": 1950
    },
    {
      "epoch": 5.81,
      "grad_norm": 0.4304823577404022,
      "learning_rate": 0.0001610916291728218,
      "loss": 0.982,
      "step": 1960
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.4701087772846222,
      "learning_rate": 0.00016072194850158756,
      "loss": 1.0111,
      "step": 1970
    },
    {
      "epoch": 5.87,
      "grad_norm": 0.527764081954956,
      "learning_rate": 0.00016035094858715064,
      "loss": 0.9659,
      "step": 1980
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.48255378007888794,
      "learning_rate": 0.00015997863748984386,
      "loss": 1.0104,
      "step": 1990
    },
    {
      "epoch": 5.93,
      "grad_norm": 0.42791295051574707,
      "learning_rate": 0.0001596050232984868,
      "loss": 1.0207,
      "step": 2000
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.47892147302627563,
      "learning_rate": 0.00015923011413020998,
      "loss": 0.9716,
      "step": 2010
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.46880820393562317,
      "learning_rate": 0.0001588539181302786,
      "loss": 1.0011,
      "step": 2020
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.49391791224479675,
      "learning_rate": 0.00015847644347191545,
      "loss": 0.9751,
      "step": 2030
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.5087260007858276,
      "learning_rate": 0.00015809769835612347,
      "loss": 0.9877,
      "step": 2040
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.5022903680801392,
      "learning_rate": 0.00015771769101150752,
      "loss": 0.9914,
      "step": 2050
    },
    {
      "epoch": 6.11,
      "grad_norm": 0.45775601267814636,
      "learning_rate": 0.00015733642969409552,
      "loss": 0.9846,
      "step": 2060
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.505850613117218,
      "learning_rate": 0.00015695392268715933,
      "loss": 0.9838,
      "step": 2070
    },
    {
      "epoch": 6.17,
      "grad_norm": 0.4375508725643158,
      "learning_rate": 0.00015657017830103447,
      "loss": 0.9828,
      "step": 2080
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.4911465346813202,
      "learning_rate": 0.0001561852048729398,
      "loss": 0.945,
      "step": 2090
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.5188708901405334,
      "learning_rate": 0.00015579901076679623,
      "loss": 0.947,
      "step": 2100
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.47982892394065857,
      "learning_rate": 0.00015541160437304523,
      "loss": 0.9443,
      "step": 2110
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.47950586676597595,
      "learning_rate": 0.00015502299410846625,
      "loss": 0.9813,
      "step": 2120
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.47209763526916504,
      "learning_rate": 0.00015463318841599406,
      "loss": 0.9921,
      "step": 2130
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.5405285954475403,
      "learning_rate": 0.00015424219576453524,
      "loss": 0.9784,
      "step": 2140
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.5008009076118469,
      "learning_rate": 0.0001538500246487843,
      "loss": 0.9955,
      "step": 2150
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.5090787410736084,
      "learning_rate": 0.00015345668358903884,
      "loss": 0.9492,
      "step": 2160
    },
    {
      "epoch": 6.43,
      "grad_norm": 0.4613390564918518,
      "learning_rate": 0.00015306218113101482,
      "loss": 0.9428,
      "step": 2170
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.4817861020565033,
      "learning_rate": 0.00015266652584566056,
      "loss": 0.9573,
      "step": 2180
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.541719377040863,
      "learning_rate": 0.00015226972632897078,
      "loss": 0.9899,
      "step": 2190
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.47960230708122253,
      "learning_rate": 0.0001518717912017997,
      "loss": 0.9913,
      "step": 2200
    },
    {
      "epoch": 6.55,
      "grad_norm": 0.5048141479492188,
      "learning_rate": 0.00015147272910967367,
      "loss": 0.9998,
      "step": 2210
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.5271062254905701,
      "learning_rate": 0.00015107254872260366,
      "loss": 1.0085,
      "step": 2220
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.4975310266017914,
      "learning_rate": 0.0001506712587348965,
      "loss": 0.9782,
      "step": 2230
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.5086869597434998,
      "learning_rate": 0.00015026886786496623,
      "loss": 0.9622,
      "step": 2240
    },
    {
      "epoch": 6.67,
      "grad_norm": 0.4691024124622345,
      "learning_rate": 0.00014986538485514466,
      "loss": 0.9635,
      "step": 2250
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.4874347448348999,
      "learning_rate": 0.00014946081847149134,
      "loss": 0.968,
      "step": 2260
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.4823012948036194,
      "learning_rate": 0.0001490551775036032,
      "loss": 0.9997,
      "step": 2270
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.5383214354515076,
      "learning_rate": 0.00014864847076442357,
      "loss": 0.9739,
      "step": 2280
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.5059105157852173,
      "learning_rate": 0.00014824070709005063,
      "loss": 1.0279,
      "step": 2290
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.4404692053794861,
      "learning_rate": 0.00014783189533954555,
      "loss": 0.9721,
      "step": 2300
    },
    {
      "epoch": 6.85,
      "grad_norm": 0.4650513231754303,
      "learning_rate": 0.00014742204439474,
      "loss": 0.9756,
      "step": 2310
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.48375043272972107,
      "learning_rate": 0.00014701116316004307,
      "loss": 0.9919,
      "step": 2320
    },
    {
      "epoch": 6.91,
      "grad_norm": 0.5243200659751892,
      "learning_rate": 0.00014659926056224796,
      "loss": 0.9987,
      "step": 2330
    },
    {
      "epoch": 6.94,
      "grad_norm": 0.47555336356163025,
      "learning_rate": 0.000146186345550338,
      "loss": 0.9958,
      "step": 2340
    },
    {
      "epoch": 6.97,
      "grad_norm": 0.4793160855770111,
      "learning_rate": 0.0001457724270952921,
      "loss": 0.9473,
      "step": 2350
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.4889741837978363,
      "learning_rate": 0.00014535751418988998,
      "loss": 0.9953,
      "step": 2360
    },
    {
      "epoch": 7.03,
      "grad_norm": 0.5149414539337158,
      "learning_rate": 0.00014494161584851687,
      "loss": 0.9294,
      "step": 2370
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.5180413722991943,
      "learning_rate": 0.0001445247411069674,
      "loss": 0.9341,
      "step": 2380
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.49888768792152405,
      "learning_rate": 0.0001441068990222495,
      "loss": 0.9638,
      "step": 2390
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.556795060634613,
      "learning_rate": 0.00014368809867238753,
      "loss": 0.8994,
      "step": 2400
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.5069085955619812,
      "learning_rate": 0.00014326834915622522,
      "loss": 0.9681,
      "step": 2410
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.5078997611999512,
      "learning_rate": 0.00014284765959322773,
      "loss": 0.9412,
      "step": 2420
    },
    {
      "epoch": 7.21,
      "grad_norm": 0.5175438523292542,
      "learning_rate": 0.00014242603912328368,
      "loss": 0.937,
      "step": 2430
    },
    {
      "epoch": 7.23,
      "grad_norm": 0.5056727528572083,
      "learning_rate": 0.00014200349690650653,
      "loss": 0.9614,
      "step": 2440
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.5162137746810913,
      "learning_rate": 0.00014158004212303565,
      "loss": 0.9487,
      "step": 2450
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.47972607612609863,
      "learning_rate": 0.0001411556839728367,
      "loss": 0.9849,
      "step": 2460
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.501246988773346,
      "learning_rate": 0.00014073043167550197,
      "loss": 0.9606,
      "step": 2470
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.47982531785964966,
      "learning_rate": 0.0001403042944700499,
      "loss": 0.9862,
      "step": 2480
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.6034518480300903,
      "learning_rate": 0.0001398772816147244,
      "loss": 0.9074,
      "step": 2490
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.5199435353279114,
      "learning_rate": 0.00013944940238679382,
      "loss": 0.937,
      "step": 2500
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.567097008228302,
      "learning_rate": 0.00013902066608234917,
      "loss": 0.9271,
      "step": 2510
    },
    {
      "epoch": 7.47,
      "grad_norm": 0.5577141642570496,
      "learning_rate": 0.00013859108201610235,
      "loss": 0.9551,
      "step": 2520
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.4936985671520233,
      "learning_rate": 0.00013816065952118367,
      "loss": 0.9517,
      "step": 2530
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.5622959136962891,
      "learning_rate": 0.00013772940794893915,
      "loss": 0.9674,
      "step": 2540
    },
    {
      "epoch": 7.56,
      "grad_norm": 0.5415753126144409,
      "learning_rate": 0.00013729733666872736,
      "loss": 0.9881,
      "step": 2550
    },
    {
      "epoch": 7.59,
      "grad_norm": 0.5343045592308044,
      "learning_rate": 0.0001368644550677157,
      "loss": 0.9624,
      "step": 2560
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.5180492997169495,
      "learning_rate": 0.00013643077255067666,
      "loss": 0.9538,
      "step": 2570
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.47799253463745117,
      "learning_rate": 0.0001359962985397834,
      "loss": 0.9726,
      "step": 2580
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.5415273308753967,
      "learning_rate": 0.00013556104247440504,
      "loss": 1.0024,
      "step": 2590
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.5413390398025513,
      "learning_rate": 0.00013512501381090158,
      "loss": 0.9833,
      "step": 2600
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.5127840042114258,
      "learning_rate": 0.00013468822202241848,
      "loss": 0.921,
      "step": 2610
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.529473066329956,
      "learning_rate": 0.00013425067659868085,
      "loss": 0.9854,
      "step": 2620
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.5207661390304565,
      "learning_rate": 0.00013381238704578718,
      "loss": 0.9833,
      "step": 2630
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.4657217562198639,
      "learning_rate": 0.00013337336288600298,
      "loss": 0.9897,
      "step": 2640
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.5837270617485046,
      "learning_rate": 0.00013293361365755374,
      "loss": 0.9852,
      "step": 2650
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.5243133306503296,
      "learning_rate": 0.0001324931489144178,
      "loss": 0.961,
      "step": 2660
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.5253849029541016,
      "learning_rate": 0.00013205197822611876,
      "loss": 0.9618,
      "step": 2670
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.5528209805488586,
      "learning_rate": 0.00013161011117751755,
      "loss": 0.938,
      "step": 2680
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.46385714411735535,
      "learning_rate": 0.00013116755736860422,
      "loss": 0.9617,
      "step": 2690
    }
  ],
  "logging_steps": 10,
  "max_steps": 6740,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "total_flos": 3.797113504029082e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
