{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 16.999258710155672,
  "eval_steps": 500,
  "global_step": 5733,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03,
      "grad_norm": 1.3075611591339111,
      "learning_rate": 0.00019999891370152373,
      "loss": 3.143,
      "step": 10
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.6303632259368896,
      "learning_rate": 0.00019999565482969577,
      "loss": 2.3399,
      "step": 20
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.3852665424346924,
      "learning_rate": 0.00019999022345531834,
      "loss": 1.7321,
      "step": 30
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.7239919900894165,
      "learning_rate": 0.00019998261969639324,
      "loss": 1.3512,
      "step": 40
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.5635300278663635,
      "learning_rate": 0.00019997284371811954,
      "loss": 1.3388,
      "step": 50
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.6905417442321777,
      "learning_rate": 0.00019996089573288983,
      "loss": 1.2353,
      "step": 60
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.5719131827354431,
      "learning_rate": 0.00019994677600028568,
      "loss": 1.204,
      "step": 70
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.7672732472419739,
      "learning_rate": 0.000199930484827072,
      "loss": 1.2053,
      "step": 80
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.5732651948928833,
      "learning_rate": 0.0001999120225671903,
      "loss": 1.1687,
      "step": 90
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5168470740318298,
      "learning_rate": 0.00019989138962175104,
      "loss": 1.2147,
      "step": 100
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6930299401283264,
      "learning_rate": 0.00019986858643902502,
      "loss": 1.1572,
      "step": 110
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.49699515104293823,
      "learning_rate": 0.00019984361351443343,
      "loss": 1.1542,
      "step": 120
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.5159195065498352,
      "learning_rate": 0.00019981647139053738,
      "loss": 1.144,
      "step": 130
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.4984678328037262,
      "learning_rate": 0.00019978716065702568,
      "loss": 1.1546,
      "step": 140
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.4763847887516022,
      "learning_rate": 0.00019975568195070253,
      "loss": 1.1143,
      "step": 150
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.5145814418792725,
      "learning_rate": 0.00019972203595547335,
      "loss": 1.107,
      "step": 160
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.5763629674911499,
      "learning_rate": 0.00019968622340232995,
      "loss": 1.1037,
      "step": 170
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.5307971835136414,
      "learning_rate": 0.0001996482450693348,
      "loss": 1.1021,
      "step": 180
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4900030493736267,
      "learning_rate": 0.000199608101781604,
      "loss": 1.1181,
      "step": 190
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.44996020197868347,
      "learning_rate": 0.00019956579441128943,
      "loss": 1.1382,
      "step": 200
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.5349639654159546,
      "learning_rate": 0.00019952132387755965,
      "loss": 1.1026,
      "step": 210
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.46941131353378296,
      "learning_rate": 0.00019947469114658017,
      "loss": 1.1339,
      "step": 220
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.6132367849349976,
      "learning_rate": 0.00019942589723149232,
      "loss": 1.1629,
      "step": 230
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.5833242535591125,
      "learning_rate": 0.00019937494319239113,
      "loss": 1.1326,
      "step": 240
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.4129420220851898,
      "learning_rate": 0.00019932183013630255,
      "loss": 1.1276,
      "step": 250
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.5098044872283936,
      "learning_rate": 0.00019926655921715923,
      "loss": 1.1047,
      "step": 260
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.48354601860046387,
      "learning_rate": 0.0001992091316357754,
      "loss": 1.1527,
      "step": 270
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.4980199337005615,
      "learning_rate": 0.00019914954863982107,
      "loss": 1.0593,
      "step": 280
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.4511754512786865,
      "learning_rate": 0.0001990878115237945,
      "loss": 1.1236,
      "step": 290
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.5281345248222351,
      "learning_rate": 0.0001990239216289944,
      "loss": 1.1307,
      "step": 300
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.49406754970550537,
      "learning_rate": 0.0001989578803434907,
      "loss": 1.1253,
      "step": 310
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.40307730436325073,
      "learning_rate": 0.00019888968910209434,
      "loss": 1.1073,
      "step": 320
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.4692971110343933,
      "learning_rate": 0.00019881934938632614,
      "loss": 1.0946,
      "step": 330
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.49514269828796387,
      "learning_rate": 0.00019874686272438466,
      "loss": 1.1216,
      "step": 340
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.4385001063346863,
      "learning_rate": 0.00019867223069111288,
      "loss": 1.0631,
      "step": 350
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.5237520933151245,
      "learning_rate": 0.00019859545490796411,
      "loss": 1.1025,
      "step": 360
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.45556092262268066,
      "learning_rate": 0.00019851653704296664,
      "loss": 1.1368,
      "step": 370
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.4565790295600891,
      "learning_rate": 0.00019843547881068764,
      "loss": 1.1072,
      "step": 380
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4629579186439514,
      "learning_rate": 0.00019835228197219573,
      "loss": 1.0693,
      "step": 390
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.484601229429245,
      "learning_rate": 0.00019826694833502295,
      "loss": 1.0472,
      "step": 400
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.5221013426780701,
      "learning_rate": 0.00019817947975312526,
      "loss": 1.0947,
      "step": 410
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.47265195846557617,
      "learning_rate": 0.00019808987812684246,
      "loss": 1.1088,
      "step": 420
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.48853638768196106,
      "learning_rate": 0.00019799814540285668,
      "loss": 1.0581,
      "step": 430
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.45954108238220215,
      "learning_rate": 0.00019790428357415032,
      "loss": 1.1373,
      "step": 440
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.4909793436527252,
      "learning_rate": 0.00019780829467996262,
      "loss": 1.0918,
      "step": 450
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.4872545599937439,
      "learning_rate": 0.00019771018080574535,
      "loss": 1.1121,
      "step": 460
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.48949578404426575,
      "learning_rate": 0.00019760994408311757,
      "loss": 1.1256,
      "step": 470
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.39528682827949524,
      "learning_rate": 0.00019750758668981924,
      "loss": 1.1134,
      "step": 480
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.5441740155220032,
      "learning_rate": 0.000197403110849664,
      "loss": 1.0463,
      "step": 490
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.48039916157722473,
      "learning_rate": 0.00019729651883249074,
      "loss": 1.1238,
      "step": 500
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.5000181794166565,
      "learning_rate": 0.00019718781295411438,
      "loss": 1.0879,
      "step": 510
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.47498947381973267,
      "learning_rate": 0.00019707699557627555,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.4218917787075043,
      "learning_rate": 0.00019696406910658918,
      "loss": 1.0946,
      "step": 530
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5329504609107971,
      "learning_rate": 0.00019684903599849233,
      "loss": 1.1378,
      "step": 540
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.5263418555259705,
      "learning_rate": 0.0001967318987511908,
      "loss": 1.0256,
      "step": 550
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.4427650272846222,
      "learning_rate": 0.00019661265990960483,
      "loss": 1.0445,
      "step": 560
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.4551290273666382,
      "learning_rate": 0.00019649132206431394,
      "loss": 1.1043,
      "step": 570
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.593726396560669,
      "learning_rate": 0.00019636788785150038,
      "loss": 1.0986,
      "step": 580
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.41312745213508606,
      "learning_rate": 0.0001962423599528921,
      "loss": 1.0529,
      "step": 590
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.46348801255226135,
      "learning_rate": 0.00019611474109570444,
      "loss": 1.0872,
      "step": 600
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.47716087102890015,
      "learning_rate": 0.00019598503405258077,
      "loss": 1.0558,
      "step": 610
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.4998435080051422,
      "learning_rate": 0.00019585324164153237,
      "loss": 1.0639,
      "step": 620
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.41655024886131287,
      "learning_rate": 0.00019571936672587716,
      "loss": 1.0767,
      "step": 630
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.41480180621147156,
      "learning_rate": 0.00019558341221417744,
      "loss": 1.1144,
      "step": 640
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.42585453391075134,
      "learning_rate": 0.0001954453810601768,
      "loss": 1.0908,
      "step": 650
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.4502318799495697,
      "learning_rate": 0.00019530527626273591,
      "loss": 1.111,
      "step": 660
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.39226168394088745,
      "learning_rate": 0.00019516310086576733,
      "loss": 1.0432,
      "step": 670
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.5012441277503967,
      "learning_rate": 0.00019501885795816936,
      "loss": 1.0818,
      "step": 680
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.44976887106895447,
      "learning_rate": 0.00019487255067375907,
      "loss": 1.008,
      "step": 690
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.42139679193496704,
      "learning_rate": 0.000194724182191204,
      "loss": 1.0488,
      "step": 700
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.5623383522033691,
      "learning_rate": 0.00019457375573395333,
      "loss": 1.0838,
      "step": 710
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.40040522813796997,
      "learning_rate": 0.00019442127457016767,
      "loss": 1.1068,
      "step": 720
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.4295685291290283,
      "learning_rate": 0.00019426674201264814,
      "loss": 1.0237,
      "step": 730
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.4583430290222168,
      "learning_rate": 0.00019411016141876437,
      "loss": 1.0699,
      "step": 740
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.5310168862342834,
      "learning_rate": 0.00019395153619038158,
      "loss": 1.0261,
      "step": 750
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.4459216892719269,
      "learning_rate": 0.00019379086977378663,
      "loss": 1.0722,
      "step": 760
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.442926824092865,
      "learning_rate": 0.00019362816565961322,
      "loss": 1.0408,
      "step": 770
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.4096292555332184,
      "learning_rate": 0.00019346342738276592,
      "loss": 1.0738,
      "step": 780
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.5359726548194885,
      "learning_rate": 0.0001932966585223436,
      "loss": 1.0907,
      "step": 790
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.43491387367248535,
      "learning_rate": 0.00019312786270156136,
      "loss": 1.0343,
      "step": 800
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.5080567598342896,
      "learning_rate": 0.0001929570435876721,
      "loss": 1.0434,
      "step": 810
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.4781583547592163,
      "learning_rate": 0.00019278420489188668,
      "loss": 1.0415,
      "step": 820
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.39566144347190857,
      "learning_rate": 0.0001926093503692933,
      "loss": 1.0718,
      "step": 830
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.46392521262168884,
      "learning_rate": 0.00019243248381877606,
      "loss": 1.073,
      "step": 840
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.5258653163909912,
      "learning_rate": 0.00019225360908293218,
      "loss": 1.0335,
      "step": 850
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.4365910291671753,
      "learning_rate": 0.00019207273004798872,
      "loss": 1.0429,
      "step": 860
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.43422266840934753,
      "learning_rate": 0.00019188985064371816,
      "loss": 1.0758,
      "step": 870
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.4220462739467621,
      "learning_rate": 0.00019170497484335278,
      "loss": 1.0639,
      "step": 880
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.40976470708847046,
      "learning_rate": 0.0001915181066634986,
      "loss": 1.0469,
      "step": 890
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.4362642467021942,
      "learning_rate": 0.00019132925016404804,
      "loss": 1.0293,
      "step": 900
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.46935397386550903,
      "learning_rate": 0.0001911384094480916,
      "loss": 1.0348,
      "step": 910
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.4438968598842621,
      "learning_rate": 0.00019094558866182893,
      "loss": 1.0386,
      "step": 920
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.471836119890213,
      "learning_rate": 0.00019075079199447846,
      "loss": 1.0528,
      "step": 930
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.40119388699531555,
      "learning_rate": 0.00019055402367818672,
      "loss": 1.0698,
      "step": 940
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.47413596510887146,
      "learning_rate": 0.0001903552879879362,
      "loss": 0.9962,
      "step": 950
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.5058668851852417,
      "learning_rate": 0.00019015458924145228,
      "loss": 1.062,
      "step": 960
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.42623311281204224,
      "learning_rate": 0.00018995193179910998,
      "loss": 1.0893,
      "step": 970
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.3833693563938141,
      "learning_rate": 0.00018974732006383862,
      "loss": 1.093,
      "step": 980
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.4372406303882599,
      "learning_rate": 0.00018954075848102658,
      "loss": 1.082,
      "step": 990
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.4586900472640991,
      "learning_rate": 0.00018933225153842446,
      "loss": 1.0778,
      "step": 1000
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.43793752789497375,
      "learning_rate": 0.00018912180376604777,
      "loss": 1.08,
      "step": 1010
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.3763284683227539,
      "learning_rate": 0.00018890941973607843,
      "loss": 1.0367,
      "step": 1020
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.4240141212940216,
      "learning_rate": 0.00018869510406276536,
      "loss": 1.0402,
      "step": 1030
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.40305736660957336,
      "learning_rate": 0.00018847886140232437,
      "loss": 1.0191,
      "step": 1040
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.4922249913215637,
      "learning_rate": 0.00018826069645283688,
      "loss": 1.0051,
      "step": 1050
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.4305824041366577,
      "learning_rate": 0.00018804061395414796,
      "loss": 1.0107,
      "step": 1060
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.4487874507904053,
      "learning_rate": 0.00018781861868776326,
      "loss": 1.0711,
      "step": 1070
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.4312799274921417,
      "learning_rate": 0.0001875947154767452,
      "loss": 1.0328,
      "step": 1080
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.39055824279785156,
      "learning_rate": 0.00018736890918560808,
      "loss": 1.0223,
      "step": 1090
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.469806432723999,
      "learning_rate": 0.0001871412047202125,
      "loss": 1.021,
      "step": 1100
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.39434587955474854,
      "learning_rate": 0.00018691160702765877,
      "loss": 1.0415,
      "step": 1110
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.42390188574790955,
      "learning_rate": 0.00018668012109617933,
      "loss": 1.0831,
      "step": 1120
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.4508221745491028,
      "learning_rate": 0.0001864467519550305,
      "loss": 1.0665,
      "step": 1130
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.456382155418396,
      "learning_rate": 0.00018621150467438308,
      "loss": 1.0191,
      "step": 1140
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.4391513764858246,
      "learning_rate": 0.0001859743843652124,
      "loss": 1.0308,
      "step": 1150
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.45625409483909607,
      "learning_rate": 0.000185735396179187,
      "loss": 1.0133,
      "step": 1160
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.4622664153575897,
      "learning_rate": 0.00018549454530855696,
      "loss": 1.036,
      "step": 1170
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.4713175296783447,
      "learning_rate": 0.00018525183698604096,
      "loss": 1.0402,
      "step": 1180
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.4186880886554718,
      "learning_rate": 0.0001850072764847126,
      "loss": 1.07,
      "step": 1190
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.4677225351333618,
      "learning_rate": 0.00018476086911788585,
      "loss": 1.0476,
      "step": 1200
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.4261798858642578,
      "learning_rate": 0.00018451262023899974,
      "loss": 1.0704,
      "step": 1210
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.4047650694847107,
      "learning_rate": 0.00018426253524150176,
      "loss": 1.0369,
      "step": 1220
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.45297375321388245,
      "learning_rate": 0.000184010619558731,
      "loss": 0.957,
      "step": 1230
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.4475320875644684,
      "learning_rate": 0.0001837568786637999,
      "loss": 1.0196,
      "step": 1240
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.4451870918273926,
      "learning_rate": 0.00018350131806947536,
      "loss": 1.0314,
      "step": 1250
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.4011303782463074,
      "learning_rate": 0.00018324394332805912,
      "loss": 1.0676,
      "step": 1260
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.4362086355686188,
      "learning_rate": 0.00018298476003126695,
      "loss": 1.0406,
      "step": 1270
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.4243050515651703,
      "learning_rate": 0.00018272377381010725,
      "loss": 1.0624,
      "step": 1280
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.4446929097175598,
      "learning_rate": 0.0001824609903347587,
      "loss": 1.0577,
      "step": 1290
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.4145580530166626,
      "learning_rate": 0.00018219641531444714,
      "loss": 1.0161,
      "step": 1300
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.48749443888664246,
      "learning_rate": 0.00018193005449732136,
      "loss": 1.042,
      "step": 1310
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.4403590261936188,
      "learning_rate": 0.0001816619136703283,
      "loss": 1.0185,
      "step": 1320
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.43967655301094055,
      "learning_rate": 0.00018139199865908743,
      "loss": 1.0225,
      "step": 1330
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.418138712644577,
      "learning_rate": 0.0001811203153277641,
      "loss": 1.033,
      "step": 1340
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.5155399441719055,
      "learning_rate": 0.00018084686957894205,
      "loss": 1.0607,
      "step": 1350
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.49232617020606995,
      "learning_rate": 0.0001805716673534953,
      "loss": 1.0296,
      "step": 1360
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.46275854110717773,
      "learning_rate": 0.000180294714630459,
      "loss": 1.0483,
      "step": 1370
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.42547962069511414,
      "learning_rate": 0.0001800160174268996,
      "loss": 1.0051,
      "step": 1380
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.41392627358436584,
      "learning_rate": 0.00017973558179778403,
      "loss": 1.0127,
      "step": 1390
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.48813772201538086,
      "learning_rate": 0.00017945341383584818,
      "loss": 1.0782,
      "step": 1400
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.45069167017936707,
      "learning_rate": 0.00017916951967146468,
      "loss": 1.0434,
      "step": 1410
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.4970340430736542,
      "learning_rate": 0.0001788839054725094,
      "loss": 0.9948,
      "step": 1420
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.4905558228492737,
      "learning_rate": 0.0001785965774442278,
      "loss": 1.0339,
      "step": 1430
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.4741889238357544,
      "learning_rate": 0.00017830754182909984,
      "loss": 1.0432,
      "step": 1440
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.4571399986743927,
      "learning_rate": 0.0001780168049067045,
      "loss": 1.0004,
      "step": 1450
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.4189036190509796,
      "learning_rate": 0.0001777243729935832,
      "loss": 1.0058,
      "step": 1460
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.4529610574245453,
      "learning_rate": 0.00017743025244310295,
      "loss": 1.0526,
      "step": 1470
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.44099265336990356,
      "learning_rate": 0.0001771344496453177,
      "loss": 1.0524,
      "step": 1480
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.40829530358314514,
      "learning_rate": 0.0001768369710268301,
      "loss": 0.9478,
      "step": 1490
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.48698168992996216,
      "learning_rate": 0.00017653782305065158,
      "loss": 1.0004,
      "step": 1500
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.44064176082611084,
      "learning_rate": 0.0001762370122160619,
      "loss": 1.0388,
      "step": 1510
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.4465246796607971,
      "learning_rate": 0.00017593454505846805,
      "loss": 0.9899,
      "step": 1520
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.5074998736381531,
      "learning_rate": 0.00017563042814926235,
      "loss": 1.0388,
      "step": 1530
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.4274372160434723,
      "learning_rate": 0.00017532466809567948,
      "loss": 0.9867,
      "step": 1540
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.4556327760219574,
      "learning_rate": 0.00017501727154065305,
      "loss": 1.0052,
      "step": 1550
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.4467003047466278,
      "learning_rate": 0.00017470824516267123,
      "loss": 0.9731,
      "step": 1560
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.5294767022132874,
      "learning_rate": 0.0001743975956756317,
      "loss": 1.0094,
      "step": 1570
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.5173203349113464,
      "learning_rate": 0.00017408532982869575,
      "loss": 1.0172,
      "step": 1580
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.4737870395183563,
      "learning_rate": 0.00017377145440614165,
      "loss": 0.9882,
      "step": 1590
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.42433103919029236,
      "learning_rate": 0.00017345597622721727,
      "loss": 1.0487,
      "step": 1600
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.4253719449043274,
      "learning_rate": 0.00017313890214599192,
      "loss": 1.0523,
      "step": 1610
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.42855310440063477,
      "learning_rate": 0.00017282023905120742,
      "loss": 0.9749,
      "step": 1620
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.4370183050632477,
      "learning_rate": 0.00017249999386612842,
      "loss": 1.0354,
      "step": 1630
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.4149131774902344,
      "learning_rate": 0.00017217817354839212,
      "loss": 0.9778,
      "step": 1640
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.39503368735313416,
      "learning_rate": 0.00017185478508985688,
      "loss": 1.0103,
      "step": 1650
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.42728662490844727,
      "learning_rate": 0.00017152983551645053,
      "loss": 1.0222,
      "step": 1660
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.4645211398601532,
      "learning_rate": 0.00017120333188801756,
      "loss": 1.0353,
      "step": 1670
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.44063910841941833,
      "learning_rate": 0.0001708752812981659,
      "loss": 1.0215,
      "step": 1680
    },
    {
      "epoch": 5.01,
      "grad_norm": 0.43445390462875366,
      "learning_rate": 0.00017054569087411264,
      "loss": 1.0503,
      "step": 1690
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.41584348678588867,
      "learning_rate": 0.00017021456777652926,
      "loss": 1.043,
      "step": 1700
    },
    {
      "epoch": 5.07,
      "grad_norm": 0.4499960243701935,
      "learning_rate": 0.00016988191919938616,
      "loss": 0.9378,
      "step": 1710
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.5038823485374451,
      "learning_rate": 0.00016954775236979614,
      "loss": 0.9954,
      "step": 1720
    },
    {
      "epoch": 5.13,
      "grad_norm": 0.4295519292354584,
      "learning_rate": 0.00016921207454785754,
      "loss": 1.0435,
      "step": 1730
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.46564555168151855,
      "learning_rate": 0.00016887489302649654,
      "loss": 0.993,
      "step": 1740
    },
    {
      "epoch": 5.19,
      "grad_norm": 0.4542476236820221,
      "learning_rate": 0.00016853621513130857,
      "loss": 0.9717,
      "step": 1750
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.42425063252449036,
      "learning_rate": 0.00016819604822039926,
      "loss": 0.9972,
      "step": 1760
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.43179255723953247,
      "learning_rate": 0.00016785439968422457,
      "loss": 1.0363,
      "step": 1770
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.47163426876068115,
      "learning_rate": 0.00016751127694543012,
      "loss": 1.0022,
      "step": 1780
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.46362826228141785,
      "learning_rate": 0.00016716668745869019,
      "loss": 1.0038,
      "step": 1790
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.4425969123840332,
      "learning_rate": 0.00016682063871054534,
      "loss": 0.9881,
      "step": 1800
    },
    {
      "epoch": 5.37,
      "grad_norm": 0.39778101444244385,
      "learning_rate": 0.00016647313821924022,
      "loss": 1.0488,
      "step": 1810
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.4620179533958435,
      "learning_rate": 0.00016612419353455988,
      "loss": 1.0063,
      "step": 1820
    },
    {
      "epoch": 5.43,
      "grad_norm": 0.46278634667396545,
      "learning_rate": 0.0001657738122376659,
      "loss": 1.0213,
      "step": 1830
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.4129514694213867,
      "learning_rate": 0.00016542200194093168,
      "loss": 0.9897,
      "step": 1840
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.4359697103500366,
      "learning_rate": 0.000165068770287777,
      "loss": 1.0218,
      "step": 1850
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.44216758012771606,
      "learning_rate": 0.00016471412495250195,
      "loss": 0.942,
      "step": 1860
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.4601258635520935,
      "learning_rate": 0.00016435807364012034,
      "loss": 0.9548,
      "step": 1870
    },
    {
      "epoch": 5.57,
      "grad_norm": 0.4706811010837555,
      "learning_rate": 0.00016400062408619207,
      "loss": 0.9416,
      "step": 1880
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.4820459187030792,
      "learning_rate": 0.00016364178405665534,
      "loss": 0.9829,
      "step": 1890
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.4250728189945221,
      "learning_rate": 0.00016328156134765763,
      "loss": 0.9873,
      "step": 1900
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.42578601837158203,
      "learning_rate": 0.00016291996378538652,
      "loss": 1.0081,
      "step": 1910
    },
    {
      "epoch": 5.69,
      "grad_norm": 0.46262380480766296,
      "learning_rate": 0.00016255699922589968,
      "loss": 0.9972,
      "step": 1920
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.5049532055854797,
      "learning_rate": 0.00016219267555495407,
      "loss": 1.023,
      "step": 1930
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.46424850821495056,
      "learning_rate": 0.00016182700068783463,
      "loss": 0.9801,
      "step": 1940
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.5047088265419006,
      "learning_rate": 0.00016145998256918238,
      "loss": 0.978,
      "step": 1950
    },
    {
      "epoch": 5.81,
      "grad_norm": 0.4304823577404022,
      "learning_rate": 0.0001610916291728218,
      "loss": 0.982,
      "step": 1960
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.4701087772846222,
      "learning_rate": 0.00016072194850158756,
      "loss": 1.0111,
      "step": 1970
    },
    {
      "epoch": 5.87,
      "grad_norm": 0.527764081954956,
      "learning_rate": 0.00016035094858715064,
      "loss": 0.9659,
      "step": 1980
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.48255378007888794,
      "learning_rate": 0.00015997863748984386,
      "loss": 1.0104,
      "step": 1990
    },
    {
      "epoch": 5.93,
      "grad_norm": 0.42791295051574707,
      "learning_rate": 0.0001596050232984868,
      "loss": 1.0207,
      "step": 2000
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.47892147302627563,
      "learning_rate": 0.00015923011413020998,
      "loss": 0.9716,
      "step": 2010
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.46880820393562317,
      "learning_rate": 0.0001588539181302786,
      "loss": 1.0011,
      "step": 2020
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.49391791224479675,
      "learning_rate": 0.00015847644347191545,
      "loss": 0.9751,
      "step": 2030
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.5087260007858276,
      "learning_rate": 0.00015809769835612347,
      "loss": 0.9877,
      "step": 2040
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.5022903680801392,
      "learning_rate": 0.00015771769101150752,
      "loss": 0.9914,
      "step": 2050
    },
    {
      "epoch": 6.11,
      "grad_norm": 0.45775601267814636,
      "learning_rate": 0.00015733642969409552,
      "loss": 0.9846,
      "step": 2060
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.505850613117218,
      "learning_rate": 0.00015695392268715933,
      "loss": 0.9838,
      "step": 2070
    },
    {
      "epoch": 6.17,
      "grad_norm": 0.4375508725643158,
      "learning_rate": 0.00015657017830103447,
      "loss": 0.9828,
      "step": 2080
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.4911465346813202,
      "learning_rate": 0.0001561852048729398,
      "loss": 0.945,
      "step": 2090
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.5188708901405334,
      "learning_rate": 0.00015579901076679623,
      "loss": 0.947,
      "step": 2100
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.47982892394065857,
      "learning_rate": 0.00015541160437304523,
      "loss": 0.9443,
      "step": 2110
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.47950586676597595,
      "learning_rate": 0.00015502299410846625,
      "loss": 0.9813,
      "step": 2120
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.47209763526916504,
      "learning_rate": 0.00015463318841599406,
      "loss": 0.9921,
      "step": 2130
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.5405285954475403,
      "learning_rate": 0.00015424219576453524,
      "loss": 0.9784,
      "step": 2140
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.5008009076118469,
      "learning_rate": 0.0001538500246487843,
      "loss": 0.9955,
      "step": 2150
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.5090787410736084,
      "learning_rate": 0.00015345668358903884,
      "loss": 0.9492,
      "step": 2160
    },
    {
      "epoch": 6.43,
      "grad_norm": 0.4613390564918518,
      "learning_rate": 0.00015306218113101482,
      "loss": 0.9428,
      "step": 2170
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.4817861020565033,
      "learning_rate": 0.00015266652584566056,
      "loss": 0.9573,
      "step": 2180
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.541719377040863,
      "learning_rate": 0.00015226972632897078,
      "loss": 0.9899,
      "step": 2190
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.47960230708122253,
      "learning_rate": 0.0001518717912017997,
      "loss": 0.9913,
      "step": 2200
    },
    {
      "epoch": 6.55,
      "grad_norm": 0.5048141479492188,
      "learning_rate": 0.00015147272910967367,
      "loss": 0.9998,
      "step": 2210
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.5271062254905701,
      "learning_rate": 0.00015107254872260366,
      "loss": 1.0085,
      "step": 2220
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.4975310266017914,
      "learning_rate": 0.0001506712587348965,
      "loss": 0.9782,
      "step": 2230
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.5086869597434998,
      "learning_rate": 0.00015026886786496623,
      "loss": 0.9622,
      "step": 2240
    },
    {
      "epoch": 6.67,
      "grad_norm": 0.4691024124622345,
      "learning_rate": 0.00014986538485514466,
      "loss": 0.9635,
      "step": 2250
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.4874347448348999,
      "learning_rate": 0.00014946081847149134,
      "loss": 0.968,
      "step": 2260
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.4823012948036194,
      "learning_rate": 0.0001490551775036032,
      "loss": 0.9997,
      "step": 2270
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.5383214354515076,
      "learning_rate": 0.00014864847076442357,
      "loss": 0.9739,
      "step": 2280
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.5059105157852173,
      "learning_rate": 0.00014824070709005063,
      "loss": 1.0279,
      "step": 2290
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.4404692053794861,
      "learning_rate": 0.00014783189533954555,
      "loss": 0.9721,
      "step": 2300
    },
    {
      "epoch": 6.85,
      "grad_norm": 0.4650513231754303,
      "learning_rate": 0.00014742204439474,
      "loss": 0.9756,
      "step": 2310
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.48375043272972107,
      "learning_rate": 0.00014701116316004307,
      "loss": 0.9919,
      "step": 2320
    },
    {
      "epoch": 6.91,
      "grad_norm": 0.5243200659751892,
      "learning_rate": 0.00014659926056224796,
      "loss": 0.9987,
      "step": 2330
    },
    {
      "epoch": 6.94,
      "grad_norm": 0.47555336356163025,
      "learning_rate": 0.000146186345550338,
      "loss": 0.9958,
      "step": 2340
    },
    {
      "epoch": 6.97,
      "grad_norm": 0.4793160855770111,
      "learning_rate": 0.0001457724270952921,
      "loss": 0.9473,
      "step": 2350
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.4889741837978363,
      "learning_rate": 0.00014535751418988998,
      "loss": 0.9953,
      "step": 2360
    },
    {
      "epoch": 7.03,
      "grad_norm": 0.5149414539337158,
      "learning_rate": 0.00014494161584851687,
      "loss": 0.9294,
      "step": 2370
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.5180413722991943,
      "learning_rate": 0.0001445247411069674,
      "loss": 0.9341,
      "step": 2380
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.49888768792152405,
      "learning_rate": 0.0001441068990222495,
      "loss": 0.9638,
      "step": 2390
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.556795060634613,
      "learning_rate": 0.00014368809867238753,
      "loss": 0.8994,
      "step": 2400
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.5069085955619812,
      "learning_rate": 0.00014326834915622522,
      "loss": 0.9681,
      "step": 2410
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.5078997611999512,
      "learning_rate": 0.00014284765959322773,
      "loss": 0.9412,
      "step": 2420
    },
    {
      "epoch": 7.21,
      "grad_norm": 0.5175438523292542,
      "learning_rate": 0.00014242603912328368,
      "loss": 0.937,
      "step": 2430
    },
    {
      "epoch": 7.23,
      "grad_norm": 0.5056727528572083,
      "learning_rate": 0.00014200349690650653,
      "loss": 0.9614,
      "step": 2440
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.5162137746810913,
      "learning_rate": 0.00014158004212303565,
      "loss": 0.9487,
      "step": 2450
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.47972607612609863,
      "learning_rate": 0.0001411556839728367,
      "loss": 0.9849,
      "step": 2460
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.501246988773346,
      "learning_rate": 0.00014073043167550197,
      "loss": 0.9606,
      "step": 2470
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.47982531785964966,
      "learning_rate": 0.0001403042944700499,
      "loss": 0.9862,
      "step": 2480
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.6034518480300903,
      "learning_rate": 0.0001398772816147244,
      "loss": 0.9074,
      "step": 2490
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.5199435353279114,
      "learning_rate": 0.00013944940238679382,
      "loss": 0.937,
      "step": 2500
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.567097008228302,
      "learning_rate": 0.00013902066608234917,
      "loss": 0.9271,
      "step": 2510
    },
    {
      "epoch": 7.47,
      "grad_norm": 0.5577141642570496,
      "learning_rate": 0.00013859108201610235,
      "loss": 0.9551,
      "step": 2520
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.4936985671520233,
      "learning_rate": 0.00013816065952118367,
      "loss": 0.9517,
      "step": 2530
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.5622959136962891,
      "learning_rate": 0.00013772940794893915,
      "loss": 0.9674,
      "step": 2540
    },
    {
      "epoch": 7.56,
      "grad_norm": 0.5415753126144409,
      "learning_rate": 0.00013729733666872736,
      "loss": 0.9881,
      "step": 2550
    },
    {
      "epoch": 7.59,
      "grad_norm": 0.5343045592308044,
      "learning_rate": 0.0001368644550677157,
      "loss": 0.9624,
      "step": 2560
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.5180492997169495,
      "learning_rate": 0.00013643077255067666,
      "loss": 0.9538,
      "step": 2570
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.47799253463745117,
      "learning_rate": 0.0001359962985397834,
      "loss": 0.9726,
      "step": 2580
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.5415273308753967,
      "learning_rate": 0.00013556104247440504,
      "loss": 1.0024,
      "step": 2590
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.5413390398025513,
      "learning_rate": 0.00013512501381090158,
      "loss": 0.9833,
      "step": 2600
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.5127840042114258,
      "learning_rate": 0.00013468822202241848,
      "loss": 0.921,
      "step": 2610
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.529473066329956,
      "learning_rate": 0.00013425067659868085,
      "loss": 0.9854,
      "step": 2620
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.5207661390304565,
      "learning_rate": 0.00013381238704578718,
      "loss": 0.9833,
      "step": 2630
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.4657217562198639,
      "learning_rate": 0.00013337336288600298,
      "loss": 0.9897,
      "step": 2640
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.5837270617485046,
      "learning_rate": 0.00013293361365755374,
      "loss": 0.9852,
      "step": 2650
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.5243133306503296,
      "learning_rate": 0.0001324931489144178,
      "loss": 0.961,
      "step": 2660
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.5253849029541016,
      "learning_rate": 0.00013205197822611876,
      "loss": 0.9618,
      "step": 2670
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.5528209805488586,
      "learning_rate": 0.00013161011117751755,
      "loss": 0.938,
      "step": 2680
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.46385714411735535,
      "learning_rate": 0.00013116755736860422,
      "loss": 0.9617,
      "step": 2690
    },
    {
      "epoch": 8.01,
      "grad_norm": 0.5097076296806335,
      "learning_rate": 0.00013072432641428932,
      "loss": 0.9572,
      "step": 2700
    },
    {
      "epoch": 8.04,
      "grad_norm": 0.5347544550895691,
      "learning_rate": 0.000130280427944195,
      "loss": 0.9482,
      "step": 2710
    },
    {
      "epoch": 8.07,
      "grad_norm": 0.522133469581604,
      "learning_rate": 0.00012983587160244603,
      "loss": 0.9301,
      "step": 2720
    },
    {
      "epoch": 8.09,
      "grad_norm": 0.5518271327018738,
      "learning_rate": 0.00012939066704745978,
      "loss": 0.9519,
      "step": 2730
    },
    {
      "epoch": 8.12,
      "grad_norm": 0.5113584399223328,
      "learning_rate": 0.00012894482395173695,
      "loss": 0.9335,
      "step": 2740
    },
    {
      "epoch": 8.15,
      "grad_norm": 0.5906926989555359,
      "learning_rate": 0.00012849835200165103,
      "loss": 0.94,
      "step": 2750
    },
    {
      "epoch": 8.18,
      "grad_norm": 0.5170208811759949,
      "learning_rate": 0.000128051260897238,
      "loss": 1.0049,
      "step": 2760
    },
    {
      "epoch": 8.21,
      "grad_norm": 0.5308312773704529,
      "learning_rate": 0.00012760356035198552,
      "loss": 0.9364,
      "step": 2770
    },
    {
      "epoch": 8.24,
      "grad_norm": 0.5613635778427124,
      "learning_rate": 0.00012715526009262208,
      "loss": 1.0066,
      "step": 2780
    },
    {
      "epoch": 8.27,
      "grad_norm": 0.5547930002212524,
      "learning_rate": 0.00012670636985890541,
      "loss": 0.9283,
      "step": 2790
    },
    {
      "epoch": 8.3,
      "grad_norm": 0.566523551940918,
      "learning_rate": 0.000126256899403411,
      "loss": 0.9396,
      "step": 2800
    },
    {
      "epoch": 8.33,
      "grad_norm": 0.5671226978302002,
      "learning_rate": 0.00012580685849132039,
      "loss": 0.9268,
      "step": 2810
    },
    {
      "epoch": 8.36,
      "grad_norm": 0.582507312297821,
      "learning_rate": 0.0001253562569002086,
      "loss": 0.8893,
      "step": 2820
    },
    {
      "epoch": 8.39,
      "grad_norm": 0.6206324696540833,
      "learning_rate": 0.00012490510441983213,
      "loss": 0.9104,
      "step": 2830
    },
    {
      "epoch": 8.42,
      "grad_norm": 0.5635004043579102,
      "learning_rate": 0.000124453410851916,
      "loss": 1.005,
      "step": 2840
    },
    {
      "epoch": 8.45,
      "grad_norm": 0.5311003923416138,
      "learning_rate": 0.0001240011860099409,
      "loss": 0.903,
      "step": 2850
    },
    {
      "epoch": 8.48,
      "grad_norm": 0.5774769186973572,
      "learning_rate": 0.00012354843971892999,
      "loss": 0.9704,
      "step": 2860
    },
    {
      "epoch": 8.51,
      "grad_norm": 0.5384257435798645,
      "learning_rate": 0.00012309518181523535,
      "loss": 0.9744,
      "step": 2870
    },
    {
      "epoch": 8.54,
      "grad_norm": 0.5624563694000244,
      "learning_rate": 0.0001226414221463244,
      "loss": 0.9463,
      "step": 2880
    },
    {
      "epoch": 8.57,
      "grad_norm": 0.5429596900939941,
      "learning_rate": 0.00012218717057056592,
      "loss": 0.8842,
      "step": 2890
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.53146892786026,
      "learning_rate": 0.00012173243695701576,
      "loss": 0.9674,
      "step": 2900
    },
    {
      "epoch": 8.63,
      "grad_norm": 0.5834932327270508,
      "learning_rate": 0.00012127723118520253,
      "loss": 0.891,
      "step": 2910
    },
    {
      "epoch": 8.66,
      "grad_norm": 0.5720138549804688,
      "learning_rate": 0.00012082156314491297,
      "loss": 0.9278,
      "step": 2920
    },
    {
      "epoch": 8.69,
      "grad_norm": 0.5216935873031616,
      "learning_rate": 0.00012036544273597707,
      "loss": 0.9372,
      "step": 2930
    },
    {
      "epoch": 8.72,
      "grad_norm": 0.5279683470726013,
      "learning_rate": 0.00011990887986805295,
      "loss": 0.9159,
      "step": 2940
    },
    {
      "epoch": 8.75,
      "grad_norm": 0.5335497856140137,
      "learning_rate": 0.00011945188446041152,
      "loss": 0.9624,
      "step": 2950
    },
    {
      "epoch": 8.78,
      "grad_norm": 0.6522032618522644,
      "learning_rate": 0.00011899446644172106,
      "loss": 0.9459,
      "step": 2960
    },
    {
      "epoch": 8.81,
      "grad_norm": 0.5878239870071411,
      "learning_rate": 0.00011853663574983154,
      "loss": 0.8794,
      "step": 2970
    },
    {
      "epoch": 8.84,
      "grad_norm": 0.7069966197013855,
      "learning_rate": 0.00011807840233155863,
      "loss": 0.884,
      "step": 2980
    },
    {
      "epoch": 8.87,
      "grad_norm": 0.582310140132904,
      "learning_rate": 0.00011761977614246757,
      "loss": 0.9692,
      "step": 2990
    },
    {
      "epoch": 8.9,
      "grad_norm": 0.49700474739074707,
      "learning_rate": 0.000117160767146657,
      "loss": 0.9235,
      "step": 3000
    },
    {
      "epoch": 8.93,
      "grad_norm": 0.652298629283905,
      "learning_rate": 0.00011670138531654238,
      "loss": 0.9472,
      "step": 3010
    },
    {
      "epoch": 8.95,
      "grad_norm": 0.560012936592102,
      "learning_rate": 0.00011624164063263932,
      "loss": 0.9171,
      "step": 3020
    },
    {
      "epoch": 8.98,
      "grad_norm": 0.5905972719192505,
      "learning_rate": 0.00011578154308334683,
      "loss": 0.9416,
      "step": 3030
    },
    {
      "epoch": 9.01,
      "grad_norm": 0.4877844750881195,
      "learning_rate": 0.00011532110266473026,
      "loss": 0.9721,
      "step": 3040
    },
    {
      "epoch": 9.04,
      "grad_norm": 0.5671539902687073,
      "learning_rate": 0.00011486032938030409,
      "loss": 0.9275,
      "step": 3050
    },
    {
      "epoch": 9.07,
      "grad_norm": 0.5479937195777893,
      "learning_rate": 0.00011439923324081466,
      "loss": 0.9297,
      "step": 3060
    },
    {
      "epoch": 9.1,
      "grad_norm": 0.595933735370636,
      "learning_rate": 0.00011393782426402267,
      "loss": 0.9552,
      "step": 3070
    },
    {
      "epoch": 9.13,
      "grad_norm": 0.5669655203819275,
      "learning_rate": 0.00011347611247448544,
      "loss": 0.982,
      "step": 3080
    },
    {
      "epoch": 9.16,
      "grad_norm": 0.545501172542572,
      "learning_rate": 0.00011301410790333928,
      "loss": 0.9092,
      "step": 3090
    },
    {
      "epoch": 9.19,
      "grad_norm": 0.6888278722763062,
      "learning_rate": 0.00011255182058808143,
      "loss": 0.9329,
      "step": 3100
    },
    {
      "epoch": 9.22,
      "grad_norm": 0.5666378736495972,
      "learning_rate": 0.00011208926057235196,
      "loss": 0.8929,
      "step": 3110
    },
    {
      "epoch": 9.25,
      "grad_norm": 0.6275875568389893,
      "learning_rate": 0.00011162643790571574,
      "loss": 0.8752,
      "step": 3120
    },
    {
      "epoch": 9.28,
      "grad_norm": 0.5310143828392029,
      "learning_rate": 0.00011116336264344389,
      "loss": 0.9242,
      "step": 3130
    },
    {
      "epoch": 9.31,
      "grad_norm": 0.5473278164863586,
      "learning_rate": 0.00011070004484629544,
      "loss": 0.9128,
      "step": 3140
    },
    {
      "epoch": 9.34,
      "grad_norm": 0.5435999035835266,
      "learning_rate": 0.00011023649458029872,
      "loss": 0.9541,
      "step": 3150
    },
    {
      "epoch": 9.37,
      "grad_norm": 0.6244938969612122,
      "learning_rate": 0.00010977272191653272,
      "loss": 0.9267,
      "step": 3160
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.6465713381767273,
      "learning_rate": 0.00010930873693090815,
      "loss": 0.9013,
      "step": 3170
    },
    {
      "epoch": 9.43,
      "grad_norm": 0.684870183467865,
      "learning_rate": 0.0001088445497039487,
      "loss": 0.935,
      "step": 3180
    },
    {
      "epoch": 9.46,
      "grad_norm": 0.5980117321014404,
      "learning_rate": 0.00010838017032057193,
      "loss": 0.9272,
      "step": 3190
    },
    {
      "epoch": 9.49,
      "grad_norm": 0.6163915991783142,
      "learning_rate": 0.00010791560886987016,
      "loss": 0.9329,
      "step": 3200
    },
    {
      "epoch": 9.52,
      "grad_norm": 0.5750844478607178,
      "learning_rate": 0.00010745087544489131,
      "loss": 0.8793,
      "step": 3210
    },
    {
      "epoch": 9.55,
      "grad_norm": 0.5583536028862,
      "learning_rate": 0.00010698598014241961,
      "loss": 0.8694,
      "step": 3220
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.6336230039596558,
      "learning_rate": 0.00010652093306275621,
      "loss": 0.9426,
      "step": 3230
    },
    {
      "epoch": 9.61,
      "grad_norm": 0.6100833415985107,
      "learning_rate": 0.00010605574430949983,
      "loss": 0.9218,
      "step": 3240
    },
    {
      "epoch": 9.64,
      "grad_norm": 0.5770401954650879,
      "learning_rate": 0.00010559042398932712,
      "loss": 0.9273,
      "step": 3250
    },
    {
      "epoch": 9.67,
      "grad_norm": 0.5431789755821228,
      "learning_rate": 0.00010512498221177318,
      "loss": 0.9602,
      "step": 3260
    },
    {
      "epoch": 9.7,
      "grad_norm": 0.6087103486061096,
      "learning_rate": 0.0001046594290890119,
      "loss": 0.9227,
      "step": 3270
    },
    {
      "epoch": 9.73,
      "grad_norm": 0.5095640420913696,
      "learning_rate": 0.00010419377473563621,
      "loss": 0.9037,
      "step": 3280
    },
    {
      "epoch": 9.76,
      "grad_norm": 0.5501900315284729,
      "learning_rate": 0.00010372802926843843,
      "loss": 0.9234,
      "step": 3290
    },
    {
      "epoch": 9.79,
      "grad_norm": 0.5776550769805908,
      "learning_rate": 0.00010326220280619037,
      "loss": 0.9461,
      "step": 3300
    },
    {
      "epoch": 9.81,
      "grad_norm": 0.5651882290840149,
      "learning_rate": 0.00010279630546942354,
      "loss": 0.8948,
      "step": 3310
    },
    {
      "epoch": 9.84,
      "grad_norm": 0.5582427382469177,
      "learning_rate": 0.00010233034738020932,
      "loss": 0.9115,
      "step": 3320
    },
    {
      "epoch": 9.87,
      "grad_norm": 0.5943388342857361,
      "learning_rate": 0.00010186433866193892,
      "loss": 0.892,
      "step": 3330
    },
    {
      "epoch": 9.9,
      "grad_norm": 0.5788332223892212,
      "learning_rate": 0.00010139828943910356,
      "loss": 0.9648,
      "step": 3340
    },
    {
      "epoch": 9.93,
      "grad_norm": 0.5527724623680115,
      "learning_rate": 0.00010093220983707448,
      "loss": 0.8949,
      "step": 3350
    },
    {
      "epoch": 9.96,
      "grad_norm": 0.7020625472068787,
      "learning_rate": 0.00010046610998188288,
      "loss": 0.9004,
      "step": 3360
    },
    {
      "epoch": 9.99,
      "grad_norm": 0.5444848537445068,
      "learning_rate": 0.0001,
      "loss": 0.8964,
      "step": 3370
    },
    {
      "epoch": 10.02,
      "grad_norm": 0.546489953994751,
      "learning_rate": 9.953389001811715e-05,
      "loss": 0.897,
      "step": 3380
    },
    {
      "epoch": 10.05,
      "grad_norm": 0.6056188941001892,
      "learning_rate": 9.906779016292555e-05,
      "loss": 0.9086,
      "step": 3390
    },
    {
      "epoch": 10.08,
      "grad_norm": 0.5762115716934204,
      "learning_rate": 9.860171056089646e-05,
      "loss": 0.8853,
      "step": 3400
    },
    {
      "epoch": 10.11,
      "grad_norm": 0.593391478061676,
      "learning_rate": 9.81356613380611e-05,
      "loss": 0.932,
      "step": 3410
    },
    {
      "epoch": 10.14,
      "grad_norm": 0.6203141808509827,
      "learning_rate": 9.76696526197907e-05,
      "loss": 0.9259,
      "step": 3420
    },
    {
      "epoch": 10.17,
      "grad_norm": 0.5954224467277527,
      "learning_rate": 9.720369453057648e-05,
      "loss": 0.8844,
      "step": 3430
    },
    {
      "epoch": 10.2,
      "grad_norm": 0.621366024017334,
      "learning_rate": 9.673779719380967e-05,
      "loss": 0.8399,
      "step": 3440
    },
    {
      "epoch": 10.23,
      "grad_norm": 0.5873314142227173,
      "learning_rate": 9.62719707315616e-05,
      "loss": 0.9542,
      "step": 3450
    },
    {
      "epoch": 10.26,
      "grad_norm": 0.6010220050811768,
      "learning_rate": 9.580622526436381e-05,
      "loss": 0.8789,
      "step": 3460
    },
    {
      "epoch": 10.29,
      "grad_norm": 0.6466110348701477,
      "learning_rate": 9.534057091098814e-05,
      "loss": 0.847,
      "step": 3470
    },
    {
      "epoch": 10.32,
      "grad_norm": 0.5997288227081299,
      "learning_rate": 9.487501778822684e-05,
      "loss": 0.9268,
      "step": 3480
    },
    {
      "epoch": 10.35,
      "grad_norm": 0.6035602688789368,
      "learning_rate": 9.440957601067293e-05,
      "loss": 0.9381,
      "step": 3490
    },
    {
      "epoch": 10.38,
      "grad_norm": 0.6124109625816345,
      "learning_rate": 9.394425569050017e-05,
      "loss": 0.8473,
      "step": 3500
    },
    {
      "epoch": 10.41,
      "grad_norm": 0.6075741648674011,
      "learning_rate": 9.347906693724378e-05,
      "loss": 0.8875,
      "step": 3510
    },
    {
      "epoch": 10.44,
      "grad_norm": 0.6938963532447815,
      "learning_rate": 9.30140198575804e-05,
      "loss": 0.91,
      "step": 3520
    },
    {
      "epoch": 10.47,
      "grad_norm": 0.644860029220581,
      "learning_rate": 9.25491245551087e-05,
      "loss": 0.9422,
      "step": 3530
    },
    {
      "epoch": 10.5,
      "grad_norm": 0.6055814027786255,
      "learning_rate": 9.208439113012984e-05,
      "loss": 0.9173,
      "step": 3540
    },
    {
      "epoch": 10.53,
      "grad_norm": 0.6469871997833252,
      "learning_rate": 9.161982967942806e-05,
      "loss": 0.8941,
      "step": 3550
    },
    {
      "epoch": 10.56,
      "grad_norm": 0.5956987738609314,
      "learning_rate": 9.115545029605129e-05,
      "loss": 0.9674,
      "step": 3560
    },
    {
      "epoch": 10.59,
      "grad_norm": 0.6006942987442017,
      "learning_rate": 9.069126306909186e-05,
      "loss": 0.8645,
      "step": 3570
    },
    {
      "epoch": 10.62,
      "grad_norm": 0.5545188188552856,
      "learning_rate": 9.022727808346732e-05,
      "loss": 0.8874,
      "step": 3580
    },
    {
      "epoch": 10.64,
      "grad_norm": 0.5852447152137756,
      "learning_rate": 8.97635054197013e-05,
      "loss": 0.874,
      "step": 3590
    },
    {
      "epoch": 10.67,
      "grad_norm": 0.7293884754180908,
      "learning_rate": 8.92999551537046e-05,
      "loss": 0.8989,
      "step": 3600
    },
    {
      "epoch": 10.7,
      "grad_norm": 0.5900417566299438,
      "learning_rate": 8.883663735655612e-05,
      "loss": 0.8654,
      "step": 3610
    },
    {
      "epoch": 10.73,
      "grad_norm": 0.6559146642684937,
      "learning_rate": 8.837356209428427e-05,
      "loss": 0.9012,
      "step": 3620
    },
    {
      "epoch": 10.76,
      "grad_norm": 0.625740110874176,
      "learning_rate": 8.791073942764806e-05,
      "loss": 0.9316,
      "step": 3630
    },
    {
      "epoch": 10.79,
      "grad_norm": 0.6410072445869446,
      "learning_rate": 8.744817941191861e-05,
      "loss": 0.9555,
      "step": 3640
    },
    {
      "epoch": 10.82,
      "grad_norm": 0.7332550883293152,
      "learning_rate": 8.698589209666074e-05,
      "loss": 0.8855,
      "step": 3650
    },
    {
      "epoch": 10.85,
      "grad_norm": 0.6180964112281799,
      "learning_rate": 8.652388752551457e-05,
      "loss": 0.9142,
      "step": 3660
    },
    {
      "epoch": 10.88,
      "grad_norm": 0.6226260662078857,
      "learning_rate": 8.606217573597737e-05,
      "loss": 0.8904,
      "step": 3670
    },
    {
      "epoch": 10.91,
      "grad_norm": 0.5866425037384033,
      "learning_rate": 8.560076675918535e-05,
      "loss": 0.8874,
      "step": 3680
    },
    {
      "epoch": 10.94,
      "grad_norm": 0.6283893585205078,
      "learning_rate": 8.513967061969595e-05,
      "loss": 0.9247,
      "step": 3690
    },
    {
      "epoch": 10.97,
      "grad_norm": 0.6214057803153992,
      "learning_rate": 8.467889733526977e-05,
      "loss": 0.9229,
      "step": 3700
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.6989147663116455,
      "learning_rate": 8.42184569166532e-05,
      "loss": 0.8957,
      "step": 3710
    },
    {
      "epoch": 11.03,
      "grad_norm": 0.6172946691513062,
      "learning_rate": 8.375835936736072e-05,
      "loss": 0.8872,
      "step": 3720
    },
    {
      "epoch": 11.06,
      "grad_norm": 0.6169922947883606,
      "learning_rate": 8.329861468345767e-05,
      "loss": 0.8919,
      "step": 3730
    },
    {
      "epoch": 11.09,
      "grad_norm": 0.5732824206352234,
      "learning_rate": 8.283923285334304e-05,
      "loss": 0.8766,
      "step": 3740
    },
    {
      "epoch": 11.12,
      "grad_norm": 0.6637181639671326,
      "learning_rate": 8.238022385753248e-05,
      "loss": 0.9,
      "step": 3750
    },
    {
      "epoch": 11.15,
      "grad_norm": 0.6639487147331238,
      "learning_rate": 8.192159766844141e-05,
      "loss": 0.9127,
      "step": 3760
    },
    {
      "epoch": 11.18,
      "grad_norm": 0.6720009446144104,
      "learning_rate": 8.146336425016848e-05,
      "loss": 0.9062,
      "step": 3770
    },
    {
      "epoch": 11.21,
      "grad_norm": 0.6730476021766663,
      "learning_rate": 8.100553355827897e-05,
      "loss": 0.8671,
      "step": 3780
    },
    {
      "epoch": 11.24,
      "grad_norm": 0.6218656897544861,
      "learning_rate": 8.054811553958852e-05,
      "loss": 0.9394,
      "step": 3790
    },
    {
      "epoch": 11.27,
      "grad_norm": 0.6198098659515381,
      "learning_rate": 8.009112013194706e-05,
      "loss": 0.9091,
      "step": 3800
    },
    {
      "epoch": 11.3,
      "grad_norm": 0.6463280320167542,
      "learning_rate": 7.963455726402291e-05,
      "loss": 0.8677,
      "step": 3810
    },
    {
      "epoch": 11.33,
      "grad_norm": 0.6925283670425415,
      "learning_rate": 7.917843685508702e-05,
      "loss": 0.8717,
      "step": 3820
    },
    {
      "epoch": 11.36,
      "grad_norm": 0.5871040225028992,
      "learning_rate": 7.872276881479749e-05,
      "loss": 0.8481,
      "step": 3830
    },
    {
      "epoch": 11.39,
      "grad_norm": 0.6593619585037231,
      "learning_rate": 7.826756304298428e-05,
      "loss": 0.8914,
      "step": 3840
    },
    {
      "epoch": 11.42,
      "grad_norm": 0.6155794262886047,
      "learning_rate": 7.78128294294341e-05,
      "loss": 0.9106,
      "step": 3850
    },
    {
      "epoch": 11.45,
      "grad_norm": 0.7153770923614502,
      "learning_rate": 7.735857785367561e-05,
      "loss": 0.8982,
      "step": 3860
    },
    {
      "epoch": 11.48,
      "grad_norm": 0.6090397834777832,
      "learning_rate": 7.690481818476467e-05,
      "loss": 0.8873,
      "step": 3870
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.6611105799674988,
      "learning_rate": 7.645156028107005e-05,
      "loss": 0.9005,
      "step": 3880
    },
    {
      "epoch": 11.53,
      "grad_norm": 0.681522786617279,
      "learning_rate": 7.599881399005913e-05,
      "loss": 0.8699,
      "step": 3890
    },
    {
      "epoch": 11.56,
      "grad_norm": 0.617684006690979,
      "learning_rate": 7.554658914808404e-05,
      "loss": 0.9232,
      "step": 3900
    },
    {
      "epoch": 11.59,
      "grad_norm": 0.5921162962913513,
      "learning_rate": 7.50948955801679e-05,
      "loss": 0.8815,
      "step": 3910
    },
    {
      "epoch": 11.62,
      "grad_norm": 0.6957433819770813,
      "learning_rate": 7.464374309979143e-05,
      "loss": 0.9039,
      "step": 3920
    },
    {
      "epoch": 11.65,
      "grad_norm": 0.6966889500617981,
      "learning_rate": 7.419314150867964e-05,
      "loss": 0.899,
      "step": 3930
    },
    {
      "epoch": 11.68,
      "grad_norm": 0.6155486702919006,
      "learning_rate": 7.3743100596589e-05,
      "loss": 0.8433,
      "step": 3940
    },
    {
      "epoch": 11.71,
      "grad_norm": 0.7069577574729919,
      "learning_rate": 7.329363014109462e-05,
      "loss": 0.8544,
      "step": 3950
    },
    {
      "epoch": 11.74,
      "grad_norm": 0.6802152991294861,
      "learning_rate": 7.284473990737794e-05,
      "loss": 0.826,
      "step": 3960
    },
    {
      "epoch": 11.77,
      "grad_norm": 0.6560751795768738,
      "learning_rate": 7.239643964801449e-05,
      "loss": 0.9058,
      "step": 3970
    },
    {
      "epoch": 11.8,
      "grad_norm": 0.7152204513549805,
      "learning_rate": 7.194873910276203e-05,
      "loss": 0.9104,
      "step": 3980
    },
    {
      "epoch": 11.83,
      "grad_norm": 0.6739766001701355,
      "learning_rate": 7.150164799834902e-05,
      "loss": 0.8791,
      "step": 3990
    },
    {
      "epoch": 11.86,
      "grad_norm": 0.6018394827842712,
      "learning_rate": 7.105517604826309e-05,
      "loss": 0.9108,
      "step": 4000
    },
    {
      "epoch": 11.89,
      "grad_norm": 0.6648141741752625,
      "learning_rate": 7.060933295254027e-05,
      "loss": 0.8949,
      "step": 4010
    },
    {
      "epoch": 11.92,
      "grad_norm": 0.6699033379554749,
      "learning_rate": 7.0164128397554e-05,
      "loss": 0.882,
      "step": 4020
    },
    {
      "epoch": 11.95,
      "grad_norm": 0.6631370186805725,
      "learning_rate": 6.971957205580498e-05,
      "loss": 0.8852,
      "step": 4030
    },
    {
      "epoch": 11.98,
      "grad_norm": 0.6701014041900635,
      "learning_rate": 6.92756735857107e-05,
      "loss": 0.8467,
      "step": 4040
    },
    {
      "epoch": 12.01,
      "grad_norm": 0.6521576642990112,
      "learning_rate": 6.883244263139578e-05,
      "loss": 0.8983,
      "step": 4050
    },
    {
      "epoch": 12.04,
      "grad_norm": 0.5832845568656921,
      "learning_rate": 6.838988882248242e-05,
      "loss": 0.8364,
      "step": 4060
    },
    {
      "epoch": 12.07,
      "grad_norm": 0.6962174773216248,
      "learning_rate": 6.794802177388124e-05,
      "loss": 0.8849,
      "step": 4070
    },
    {
      "epoch": 12.1,
      "grad_norm": 0.6368464231491089,
      "learning_rate": 6.750685108558222e-05,
      "loss": 0.8236,
      "step": 4080
    },
    {
      "epoch": 12.13,
      "grad_norm": 0.6341462135314941,
      "learning_rate": 6.706638634244629e-05,
      "loss": 0.8633,
      "step": 4090
    },
    {
      "epoch": 12.16,
      "grad_norm": 0.6459080576896667,
      "learning_rate": 6.662663711399705e-05,
      "loss": 0.8445,
      "step": 4100
    },
    {
      "epoch": 12.19,
      "grad_norm": 0.7153635621070862,
      "learning_rate": 6.618761295421284e-05,
      "loss": 0.8896,
      "step": 4110
    },
    {
      "epoch": 12.22,
      "grad_norm": 0.6469669342041016,
      "learning_rate": 6.574932340131918e-05,
      "loss": 0.8364,
      "step": 4120
    },
    {
      "epoch": 12.25,
      "grad_norm": 0.6536933183670044,
      "learning_rate": 6.531177797758154e-05,
      "loss": 0.8537,
      "step": 4130
    },
    {
      "epoch": 12.28,
      "grad_norm": 0.6263059973716736,
      "learning_rate": 6.487498618909845e-05,
      "loss": 0.867,
      "step": 4140
    },
    {
      "epoch": 12.31,
      "grad_norm": 0.6706723570823669,
      "learning_rate": 6.443895752559498e-05,
      "loss": 0.9006,
      "step": 4150
    },
    {
      "epoch": 12.34,
      "grad_norm": 0.6973928809165955,
      "learning_rate": 6.400370146021662e-05,
      "loss": 0.8917,
      "step": 4160
    },
    {
      "epoch": 12.36,
      "grad_norm": 0.6501586437225342,
      "learning_rate": 6.356922744932335e-05,
      "loss": 0.8938,
      "step": 4170
    },
    {
      "epoch": 12.39,
      "grad_norm": 0.6242647171020508,
      "learning_rate": 6.313554493228431e-05,
      "loss": 0.8624,
      "step": 4180
    },
    {
      "epoch": 12.42,
      "grad_norm": 0.645351231098175,
      "learning_rate": 6.270266333127267e-05,
      "loss": 0.8641,
      "step": 4190
    },
    {
      "epoch": 12.45,
      "grad_norm": 0.7022139430046082,
      "learning_rate": 6.227059205106084e-05,
      "loss": 0.8685,
      "step": 4200
    },
    {
      "epoch": 12.48,
      "grad_norm": 0.7805647850036621,
      "learning_rate": 6.183934047881635e-05,
      "loss": 0.8675,
      "step": 4210
    },
    {
      "epoch": 12.51,
      "grad_norm": 0.6161932945251465,
      "learning_rate": 6.14089179838977e-05,
      "loss": 0.833,
      "step": 4220
    },
    {
      "epoch": 12.54,
      "grad_norm": 0.701529324054718,
      "learning_rate": 6.0979333917650875e-05,
      "loss": 0.8652,
      "step": 4230
    },
    {
      "epoch": 12.57,
      "grad_norm": 0.7353037595748901,
      "learning_rate": 6.05505976132062e-05,
      "loss": 0.8773,
      "step": 4240
    },
    {
      "epoch": 12.6,
      "grad_norm": 0.6741099953651428,
      "learning_rate": 6.0122718385275615e-05,
      "loss": 0.8899,
      "step": 4250
    },
    {
      "epoch": 12.63,
      "grad_norm": 0.6937631964683533,
      "learning_rate": 5.969570552995014e-05,
      "loss": 0.8892,
      "step": 4260
    },
    {
      "epoch": 12.66,
      "grad_norm": 0.6463007926940918,
      "learning_rate": 5.9269568324498056e-05,
      "loss": 0.8774,
      "step": 4270
    },
    {
      "epoch": 12.69,
      "grad_norm": 0.6981810927391052,
      "learning_rate": 5.8844316027163315e-05,
      "loss": 0.9043,
      "step": 4280
    },
    {
      "epoch": 12.72,
      "grad_norm": 0.7068877816200256,
      "learning_rate": 5.841995787696438e-05,
      "loss": 0.9522,
      "step": 4290
    },
    {
      "epoch": 12.75,
      "grad_norm": 0.6823083758354187,
      "learning_rate": 5.799650309349348e-05,
      "loss": 0.844,
      "step": 4300
    },
    {
      "epoch": 12.78,
      "grad_norm": 0.6759798526763916,
      "learning_rate": 5.7573960876716335e-05,
      "loss": 0.865,
      "step": 4310
    },
    {
      "epoch": 12.81,
      "grad_norm": 0.7221011519432068,
      "learning_rate": 5.715234040677229e-05,
      "loss": 0.8673,
      "step": 4320
    },
    {
      "epoch": 12.84,
      "grad_norm": 0.6951500177383423,
      "learning_rate": 5.6731650843774785e-05,
      "loss": 0.8715,
      "step": 4330
    },
    {
      "epoch": 12.87,
      "grad_norm": 0.657943069934845,
      "learning_rate": 5.631190132761247e-05,
      "loss": 0.8964,
      "step": 4340
    },
    {
      "epoch": 12.9,
      "grad_norm": 0.653073251247406,
      "learning_rate": 5.589310097775054e-05,
      "loss": 0.8832,
      "step": 4350
    },
    {
      "epoch": 12.93,
      "grad_norm": 0.6650052070617676,
      "learning_rate": 5.547525889303264e-05,
      "loss": 0.8695,
      "step": 4360
    },
    {
      "epoch": 12.96,
      "grad_norm": 0.6671644449234009,
      "learning_rate": 5.5058384151483166e-05,
      "loss": 0.9067,
      "step": 4370
    },
    {
      "epoch": 12.99,
      "grad_norm": 0.6838709712028503,
      "learning_rate": 5.4642485810110025e-05,
      "loss": 0.9021,
      "step": 4380
    },
    {
      "epoch": 13.02,
      "grad_norm": 0.633551299571991,
      "learning_rate": 5.422757290470795e-05,
      "loss": 0.8728,
      "step": 4390
    },
    {
      "epoch": 13.05,
      "grad_norm": 0.6814026832580566,
      "learning_rate": 5.381365444966204e-05,
      "loss": 0.8576,
      "step": 4400
    },
    {
      "epoch": 13.08,
      "grad_norm": 0.6865212917327881,
      "learning_rate": 5.3400739437752053e-05,
      "loss": 0.8653,
      "step": 4410
    },
    {
      "epoch": 13.11,
      "grad_norm": 0.7169237732887268,
      "learning_rate": 5.2988836839956965e-05,
      "loss": 0.8628,
      "step": 4420
    },
    {
      "epoch": 13.14,
      "grad_norm": 0.635223925113678,
      "learning_rate": 5.2577955605260044e-05,
      "loss": 0.825,
      "step": 4430
    },
    {
      "epoch": 13.17,
      "grad_norm": 0.693120002746582,
      "learning_rate": 5.2168104660454476e-05,
      "loss": 0.8881,
      "step": 4440
    },
    {
      "epoch": 13.19,
      "grad_norm": 0.6808448433876038,
      "learning_rate": 5.1759292909949405e-05,
      "loss": 0.9134,
      "step": 4450
    },
    {
      "epoch": 13.22,
      "grad_norm": 0.6411047577857971,
      "learning_rate": 5.135152923557647e-05,
      "loss": 0.8763,
      "step": 4460
    },
    {
      "epoch": 13.25,
      "grad_norm": 0.6144923567771912,
      "learning_rate": 5.094482249639683e-05,
      "loss": 0.8368,
      "step": 4470
    },
    {
      "epoch": 13.28,
      "grad_norm": 0.7774910926818848,
      "learning_rate": 5.053918152850868e-05,
      "loss": 0.8525,
      "step": 4480
    },
    {
      "epoch": 13.31,
      "grad_norm": 0.6396511197090149,
      "learning_rate": 5.013461514485536e-05,
      "loss": 0.8376,
      "step": 4490
    },
    {
      "epoch": 13.34,
      "grad_norm": 0.7194623351097107,
      "learning_rate": 4.9731132135033786e-05,
      "loss": 0.8194,
      "step": 4500
    },
    {
      "epoch": 13.37,
      "grad_norm": 0.7330255508422852,
      "learning_rate": 4.932874126510353e-05,
      "loss": 0.8894,
      "step": 4510
    },
    {
      "epoch": 13.4,
      "grad_norm": 0.6667584180831909,
      "learning_rate": 4.8927451277396366e-05,
      "loss": 0.8567,
      "step": 4520
    },
    {
      "epoch": 13.43,
      "grad_norm": 0.6781356334686279,
      "learning_rate": 4.8527270890326336e-05,
      "loss": 0.8283,
      "step": 4530
    },
    {
      "epoch": 13.46,
      "grad_norm": 0.7297731041908264,
      "learning_rate": 4.812820879820034e-05,
      "loss": 0.815,
      "step": 4540
    },
    {
      "epoch": 13.49,
      "grad_norm": 0.6583125591278076,
      "learning_rate": 4.773027367102924e-05,
      "loss": 0.8997,
      "step": 4550
    },
    {
      "epoch": 13.52,
      "grad_norm": 0.7163435816764832,
      "learning_rate": 4.733347415433945e-05,
      "loss": 0.8649,
      "step": 4560
    },
    {
      "epoch": 13.55,
      "grad_norm": 0.6818379163742065,
      "learning_rate": 4.693781886898521e-05,
      "loss": 0.8643,
      "step": 4570
    },
    {
      "epoch": 13.58,
      "grad_norm": 0.7568011283874512,
      "learning_rate": 4.654331641096118e-05,
      "loss": 0.8279,
      "step": 4580
    },
    {
      "epoch": 13.61,
      "grad_norm": 0.7222955822944641,
      "learning_rate": 4.6149975351215735e-05,
      "loss": 0.8647,
      "step": 4590
    },
    {
      "epoch": 13.64,
      "grad_norm": 0.7273449301719666,
      "learning_rate": 4.575780423546476e-05,
      "loss": 0.8292,
      "step": 4600
    },
    {
      "epoch": 13.67,
      "grad_norm": 0.7367998361587524,
      "learning_rate": 4.536681158400597e-05,
      "loss": 0.8563,
      "step": 4610
    },
    {
      "epoch": 13.7,
      "grad_norm": 0.7967364192008972,
      "learning_rate": 4.497700589153379e-05,
      "loss": 0.8665,
      "step": 4620
    },
    {
      "epoch": 13.73,
      "grad_norm": 0.6993856430053711,
      "learning_rate": 4.45883956269548e-05,
      "loss": 0.8539,
      "step": 4630
    },
    {
      "epoch": 13.76,
      "grad_norm": 0.7850034832954407,
      "learning_rate": 4.4200989233203785e-05,
      "loss": 0.8949,
      "step": 4640
    },
    {
      "epoch": 13.79,
      "grad_norm": 0.7437024116516113,
      "learning_rate": 4.381479512706025e-05,
      "loss": 0.8996,
      "step": 4650
    },
    {
      "epoch": 13.82,
      "grad_norm": 0.6639114618301392,
      "learning_rate": 4.342982169896556e-05,
      "loss": 0.8396,
      "step": 4660
    },
    {
      "epoch": 13.85,
      "grad_norm": 0.7372486591339111,
      "learning_rate": 4.3046077312840694e-05,
      "loss": 0.8905,
      "step": 4670
    },
    {
      "epoch": 13.88,
      "grad_norm": 0.7259857654571533,
      "learning_rate": 4.2663570305904486e-05,
      "loss": 0.8507,
      "step": 4680
    },
    {
      "epoch": 13.91,
      "grad_norm": 0.7406822443008423,
      "learning_rate": 4.228230898849253e-05,
      "loss": 0.8939,
      "step": 4690
    },
    {
      "epoch": 13.94,
      "grad_norm": 0.7093822360038757,
      "learning_rate": 4.190230164387655e-05,
      "loss": 0.8935,
      "step": 4700
    },
    {
      "epoch": 13.97,
      "grad_norm": 0.7266072630882263,
      "learning_rate": 4.152355652808457e-05,
      "loss": 0.8556,
      "step": 4710
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.6781320571899414,
      "learning_rate": 4.114608186972143e-05,
      "loss": 0.8372,
      "step": 4720
    },
    {
      "epoch": 14.03,
      "grad_norm": 0.7091798186302185,
      "learning_rate": 4.0769885869790035e-05,
      "loss": 0.8428,
      "step": 4730
    },
    {
      "epoch": 14.05,
      "grad_norm": 0.7385082244873047,
      "learning_rate": 4.039497670151323e-05,
      "loss": 0.8518,
      "step": 4740
    },
    {
      "epoch": 14.08,
      "grad_norm": 0.7321718335151672,
      "learning_rate": 4.0021362510156166e-05,
      "loss": 0.8263,
      "step": 4750
    },
    {
      "epoch": 14.11,
      "grad_norm": 0.7442460060119629,
      "learning_rate": 3.96490514128494e-05,
      "loss": 0.8283,
      "step": 4760
    },
    {
      "epoch": 14.14,
      "grad_norm": 0.6659286022186279,
      "learning_rate": 3.9278051498412474e-05,
      "loss": 0.7944,
      "step": 4770
    },
    {
      "epoch": 14.17,
      "grad_norm": 0.7307924032211304,
      "learning_rate": 3.890837082717822e-05,
      "loss": 0.8476,
      "step": 4780
    },
    {
      "epoch": 14.2,
      "grad_norm": 0.7305827140808105,
      "learning_rate": 3.854001743081764e-05,
      "loss": 0.8422,
      "step": 4790
    },
    {
      "epoch": 14.23,
      "grad_norm": 0.7306708097457886,
      "learning_rate": 3.817299931216537e-05,
      "loss": 0.8435,
      "step": 4800
    },
    {
      "epoch": 14.26,
      "grad_norm": 0.7496688365936279,
      "learning_rate": 3.780732444504592e-05,
      "loss": 0.8625,
      "step": 4810
    },
    {
      "epoch": 14.29,
      "grad_norm": 0.6919867992401123,
      "learning_rate": 3.74430007741003e-05,
      "loss": 0.853,
      "step": 4820
    },
    {
      "epoch": 14.32,
      "grad_norm": 0.7097910642623901,
      "learning_rate": 3.7080036214613465e-05,
      "loss": 0.884,
      "step": 4830
    },
    {
      "epoch": 14.35,
      "grad_norm": 0.7217890024185181,
      "learning_rate": 3.671843865234238e-05,
      "loss": 0.8265,
      "step": 4840
    },
    {
      "epoch": 14.38,
      "grad_norm": 0.6916340589523315,
      "learning_rate": 3.635821594334467e-05,
      "loss": 0.8477,
      "step": 4850
    },
    {
      "epoch": 14.41,
      "grad_norm": 0.7887072563171387,
      "learning_rate": 3.5999375913807907e-05,
      "loss": 0.9072,
      "step": 4860
    },
    {
      "epoch": 14.44,
      "grad_norm": 0.7604262828826904,
      "learning_rate": 3.564192635987966e-05,
      "loss": 0.8819,
      "step": 4870
    },
    {
      "epoch": 14.47,
      "grad_norm": 0.7523472309112549,
      "learning_rate": 3.5285875047498076e-05,
      "loss": 0.8479,
      "step": 4880
    },
    {
      "epoch": 14.5,
      "grad_norm": 0.6998950839042664,
      "learning_rate": 3.493122971222305e-05,
      "loss": 0.8008,
      "step": 4890
    },
    {
      "epoch": 14.53,
      "grad_norm": 0.745513916015625,
      "learning_rate": 3.457799805906835e-05,
      "loss": 0.8697,
      "step": 4900
    },
    {
      "epoch": 14.56,
      "grad_norm": 0.7075234055519104,
      "learning_rate": 3.422618776233413e-05,
      "loss": 0.8803,
      "step": 4910
    },
    {
      "epoch": 14.59,
      "grad_norm": 0.7295088768005371,
      "learning_rate": 3.3875806465440154e-05,
      "loss": 0.8361,
      "step": 4920
    },
    {
      "epoch": 14.62,
      "grad_norm": 0.8171834945678711,
      "learning_rate": 3.352686178075981e-05,
      "loss": 0.8679,
      "step": 4930
    },
    {
      "epoch": 14.65,
      "grad_norm": 0.7186451554298401,
      "learning_rate": 3.317936128945469e-05,
      "loss": 0.8494,
      "step": 4940
    },
    {
      "epoch": 14.68,
      "grad_norm": 0.7498088479042053,
      "learning_rate": 3.283331254130987e-05,
      "loss": 0.8554,
      "step": 4950
    },
    {
      "epoch": 14.71,
      "grad_norm": 0.7182351350784302,
      "learning_rate": 3.2488723054569904e-05,
      "loss": 0.8412,
      "step": 4960
    },
    {
      "epoch": 14.74,
      "grad_norm": 0.6885325908660889,
      "learning_rate": 3.214560031577548e-05,
      "loss": 0.8584,
      "step": 4970
    },
    {
      "epoch": 14.77,
      "grad_norm": 0.7613860964775085,
      "learning_rate": 3.180395177960077e-05,
      "loss": 0.8642,
      "step": 4980
    },
    {
      "epoch": 14.8,
      "grad_norm": 0.7366929650306702,
      "learning_rate": 3.146378486869146e-05,
      "loss": 0.843,
      "step": 4990
    },
    {
      "epoch": 14.83,
      "grad_norm": 0.8083546757698059,
      "learning_rate": 3.1125106973503485e-05,
      "loss": 0.8599,
      "step": 5000
    },
    {
      "epoch": 14.86,
      "grad_norm": 0.7575716376304626,
      "learning_rate": 3.078792545214247e-05,
      "loss": 0.8529,
      "step": 5010
    },
    {
      "epoch": 14.89,
      "grad_norm": 0.7348952889442444,
      "learning_rate": 3.0452247630203902e-05,
      "loss": 0.8392,
      "step": 5020
    },
    {
      "epoch": 14.91,
      "grad_norm": 0.6707155108451843,
      "learning_rate": 3.011808080061387e-05,
      "loss": 0.8523,
      "step": 5030
    },
    {
      "epoch": 14.94,
      "grad_norm": 0.7190436124801636,
      "learning_rate": 2.9785432223470756e-05,
      "loss": 0.8481,
      "step": 5040
    },
    {
      "epoch": 14.97,
      "grad_norm": 0.788718044757843,
      "learning_rate": 2.9454309125887404e-05,
      "loss": 0.9038,
      "step": 5050
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.7284851670265198,
      "learning_rate": 2.9124718701834107e-05,
      "loss": 0.8588,
      "step": 5060
    },
    {
      "epoch": 15.03,
      "grad_norm": 0.81318598985672,
      "learning_rate": 2.879666811198244e-05,
      "loss": 0.8802,
      "step": 5070
    },
    {
      "epoch": 15.06,
      "grad_norm": 0.735966682434082,
      "learning_rate": 2.847016448354948e-05,
      "loss": 0.8603,
      "step": 5080
    },
    {
      "epoch": 15.09,
      "grad_norm": 0.7405813932418823,
      "learning_rate": 2.8145214910143124e-05,
      "loss": 0.8359,
      "step": 5090
    },
    {
      "epoch": 15.12,
      "grad_norm": 0.6764246225357056,
      "learning_rate": 2.782182645160789e-05,
      "loss": 0.8046,
      "step": 5100
    },
    {
      "epoch": 15.15,
      "grad_norm": 0.7175901532173157,
      "learning_rate": 2.750000613387157e-05,
      "loss": 0.8607,
      "step": 5110
    },
    {
      "epoch": 15.18,
      "grad_norm": 0.7289192080497742,
      "learning_rate": 2.71797609487926e-05,
      "loss": 0.8514,
      "step": 5120
    },
    {
      "epoch": 15.21,
      "grad_norm": 0.7827337980270386,
      "learning_rate": 2.686109785400809e-05,
      "loss": 0.8525,
      "step": 5130
    },
    {
      "epoch": 15.24,
      "grad_norm": 0.7363079786300659,
      "learning_rate": 2.6544023772782732e-05,
      "loss": 0.8554,
      "step": 5140
    },
    {
      "epoch": 15.27,
      "grad_norm": 0.7272961735725403,
      "learning_rate": 2.6228545593858355e-05,
      "loss": 0.8707,
      "step": 5150
    },
    {
      "epoch": 15.3,
      "grad_norm": 0.842999279499054,
      "learning_rate": 2.5914670171304257e-05,
      "loss": 0.8849,
      "step": 5160
    },
    {
      "epoch": 15.33,
      "grad_norm": 0.786648690700531,
      "learning_rate": 2.560240432436831e-05,
      "loss": 0.8897,
      "step": 5170
    },
    {
      "epoch": 15.36,
      "grad_norm": 0.7102116346359253,
      "learning_rate": 2.5291754837328784e-05,
      "loss": 0.7969,
      "step": 5180
    },
    {
      "epoch": 15.39,
      "grad_norm": 0.693680465221405,
      "learning_rate": 2.498272845934697e-05,
      "loss": 0.8447,
      "step": 5190
    },
    {
      "epoch": 15.42,
      "grad_norm": 0.7600116729736328,
      "learning_rate": 2.4675331904320532e-05,
      "loss": 0.8381,
      "step": 5200
    },
    {
      "epoch": 15.45,
      "grad_norm": 0.7859331369400024,
      "learning_rate": 2.436957185073766e-05,
      "loss": 0.8395,
      "step": 5210
    },
    {
      "epoch": 15.48,
      "grad_norm": 0.7398810982704163,
      "learning_rate": 2.406545494153196e-05,
      "loss": 0.8828,
      "step": 5220
    },
    {
      "epoch": 15.51,
      "grad_norm": 0.6785945296287537,
      "learning_rate": 2.376298778393814e-05,
      "loss": 0.864,
      "step": 5230
    },
    {
      "epoch": 15.54,
      "grad_norm": 0.674453854560852,
      "learning_rate": 2.3462176949348467e-05,
      "loss": 0.8064,
      "step": 5240
    },
    {
      "epoch": 15.57,
      "grad_norm": 0.7183998227119446,
      "learning_rate": 2.316302897316992e-05,
      "loss": 0.8267,
      "step": 5250
    },
    {
      "epoch": 15.6,
      "grad_norm": 0.7314171195030212,
      "learning_rate": 2.286555035468233e-05,
      "loss": 0.8408,
      "step": 5260
    },
    {
      "epoch": 15.63,
      "grad_norm": 0.7401379346847534,
      "learning_rate": 2.2569747556897102e-05,
      "loss": 0.8248,
      "step": 5270
    },
    {
      "epoch": 15.66,
      "grad_norm": 0.7889806628227234,
      "learning_rate": 2.22756270064168e-05,
      "loss": 0.8289,
      "step": 5280
    },
    {
      "epoch": 15.69,
      "grad_norm": 0.7863678932189941,
      "learning_rate": 2.1983195093295563e-05,
      "loss": 0.835,
      "step": 5290
    },
    {
      "epoch": 15.72,
      "grad_norm": 0.7995879054069519,
      "learning_rate": 2.1692458170900198e-05,
      "loss": 0.8477,
      "step": 5300
    },
    {
      "epoch": 15.74,
      "grad_norm": 0.9345439672470093,
      "learning_rate": 2.1403422555772224e-05,
      "loss": 0.8695,
      "step": 5310
    },
    {
      "epoch": 15.77,
      "grad_norm": 0.7543356418609619,
      "learning_rate": 2.1116094527490594e-05,
      "loss": 0.8386,
      "step": 5320
    },
    {
      "epoch": 15.8,
      "grad_norm": 0.7049757838249207,
      "learning_rate": 2.083048032853534e-05,
      "loss": 0.8467,
      "step": 5330
    },
    {
      "epoch": 15.83,
      "grad_norm": 0.783416748046875,
      "learning_rate": 2.0546586164151827e-05,
      "loss": 0.8122,
      "step": 5340
    },
    {
      "epoch": 15.86,
      "grad_norm": 0.8148676753044128,
      "learning_rate": 2.0264418202215995e-05,
      "loss": 0.8194,
      "step": 5350
    },
    {
      "epoch": 15.89,
      "grad_norm": 0.7577230930328369,
      "learning_rate": 1.9983982573100413e-05,
      "loss": 0.8566,
      "step": 5360
    },
    {
      "epoch": 15.92,
      "grad_norm": 0.7965112328529358,
      "learning_rate": 1.9705285369540995e-05,
      "loss": 0.8467,
      "step": 5370
    },
    {
      "epoch": 15.95,
      "grad_norm": 0.8069000840187073,
      "learning_rate": 1.94283326465047e-05,
      "loss": 0.813,
      "step": 5380
    },
    {
      "epoch": 15.98,
      "grad_norm": 0.7559773325920105,
      "learning_rate": 1.9153130421057954e-05,
      "loss": 0.8163,
      "step": 5390
    },
    {
      "epoch": 16.01,
      "grad_norm": 0.746700644493103,
      "learning_rate": 1.887968467223591e-05,
      "loss": 0.8455,
      "step": 5400
    },
    {
      "epoch": 16.04,
      "grad_norm": 0.8091588020324707,
      "learning_rate": 1.8608001340912572e-05,
      "loss": 0.8137,
      "step": 5410
    },
    {
      "epoch": 16.07,
      "grad_norm": 0.7903063297271729,
      "learning_rate": 1.8338086329671733e-05,
      "loss": 0.8724,
      "step": 5420
    },
    {
      "epoch": 16.1,
      "grad_norm": 0.7495721578598022,
      "learning_rate": 1.8069945502678687e-05,
      "loss": 0.8292,
      "step": 5430
    },
    {
      "epoch": 16.13,
      "grad_norm": 0.7426685690879822,
      "learning_rate": 1.7803584685552875e-05,
      "loss": 0.8513,
      "step": 5440
    },
    {
      "epoch": 16.16,
      "grad_norm": 0.6754805445671082,
      "learning_rate": 1.753900966524129e-05,
      "loss": 0.8198,
      "step": 5450
    },
    {
      "epoch": 16.19,
      "grad_norm": 0.7824282050132751,
      "learning_rate": 1.7276226189892762e-05,
      "loss": 0.8248,
      "step": 5460
    },
    {
      "epoch": 16.22,
      "grad_norm": 0.8116416335105896,
      "learning_rate": 1.7015239968733065e-05,
      "loss": 0.8817,
      "step": 5470
    },
    {
      "epoch": 16.25,
      "grad_norm": 0.8335505127906799,
      "learning_rate": 1.67560566719409e-05,
      "loss": 0.8299,
      "step": 5480
    },
    {
      "epoch": 16.28,
      "grad_norm": 0.7427066564559937,
      "learning_rate": 1.6498681930524653e-05,
      "loss": 0.8376,
      "step": 5490
    },
    {
      "epoch": 16.31,
      "grad_norm": 0.8231301307678223,
      "learning_rate": 1.6243121336200127e-05,
      "loss": 0.8212,
      "step": 5500
    },
    {
      "epoch": 16.34,
      "grad_norm": 0.7606331706047058,
      "learning_rate": 1.598938044126901e-05,
      "loss": 0.866,
      "step": 5510
    },
    {
      "epoch": 16.37,
      "grad_norm": 0.8000193238258362,
      "learning_rate": 1.5737464758498243e-05,
      "loss": 0.8319,
      "step": 5520
    },
    {
      "epoch": 16.4,
      "grad_norm": 0.7731227874755859,
      "learning_rate": 1.5487379761000274e-05,
      "loss": 0.8481,
      "step": 5530
    },
    {
      "epoch": 16.43,
      "grad_norm": 0.8450667858123779,
      "learning_rate": 1.523913088211415e-05,
      "loss": 0.8576,
      "step": 5540
    },
    {
      "epoch": 16.46,
      "grad_norm": 0.7721903324127197,
      "learning_rate": 1.499272351528742e-05,
      "loss": 0.8445,
      "step": 5550
    },
    {
      "epoch": 16.49,
      "grad_norm": 0.7212638258934021,
      "learning_rate": 1.474816301395906e-05,
      "loss": 0.8144,
      "step": 5560
    },
    {
      "epoch": 16.52,
      "grad_norm": 0.8217735290527344,
      "learning_rate": 1.4505454691443043e-05,
      "loss": 0.8148,
      "step": 5570
    },
    {
      "epoch": 16.55,
      "grad_norm": 0.7749047875404358,
      "learning_rate": 1.4264603820813006e-05,
      "loss": 0.8182,
      "step": 5580
    },
    {
      "epoch": 16.58,
      "grad_norm": 0.7260986566543579,
      "learning_rate": 1.4025615634787615e-05,
      "loss": 0.8584,
      "step": 5590
    },
    {
      "epoch": 16.6,
      "grad_norm": 0.8712196350097656,
      "learning_rate": 1.3788495325616912e-05,
      "loss": 0.8618,
      "step": 5600
    },
    {
      "epoch": 16.63,
      "grad_norm": 0.7554131746292114,
      "learning_rate": 1.3553248044969524e-05,
      "loss": 0.8061,
      "step": 5610
    },
    {
      "epoch": 16.66,
      "grad_norm": 0.7792167663574219,
      "learning_rate": 1.3319878903820681e-05,
      "loss": 0.8247,
      "step": 5620
    },
    {
      "epoch": 16.69,
      "grad_norm": 0.7688063383102417,
      "learning_rate": 1.3088392972341256e-05,
      "loss": 0.8521,
      "step": 5630
    },
    {
      "epoch": 16.72,
      "grad_norm": 0.7950536608695984,
      "learning_rate": 1.2858795279787516e-05,
      "loss": 0.8376,
      "step": 5640
    },
    {
      "epoch": 16.75,
      "grad_norm": 0.7813002467155457,
      "learning_rate": 1.2631090814391944e-05,
      "loss": 0.8261,
      "step": 5650
    },
    {
      "epoch": 16.78,
      "grad_norm": 0.7560390830039978,
      "learning_rate": 1.2405284523254823e-05,
      "loss": 0.8194,
      "step": 5660
    },
    {
      "epoch": 16.81,
      "grad_norm": 0.7095327377319336,
      "learning_rate": 1.218138131223675e-05,
      "loss": 0.859,
      "step": 5670
    },
    {
      "epoch": 16.84,
      "grad_norm": 0.8147637844085693,
      "learning_rate": 1.195938604585205e-05,
      "loss": 0.8712,
      "step": 5680
    },
    {
      "epoch": 16.87,
      "grad_norm": 0.6999373435974121,
      "learning_rate": 1.173930354716314e-05,
      "loss": 0.8489,
      "step": 5690
    },
    {
      "epoch": 16.9,
      "grad_norm": 0.8096385598182678,
      "learning_rate": 1.1521138597675651e-05,
      "loss": 0.8575,
      "step": 5700
    },
    {
      "epoch": 16.93,
      "grad_norm": 0.8211979866027832,
      "learning_rate": 1.130489593723465e-05,
      "loss": 0.8374,
      "step": 5710
    },
    {
      "epoch": 16.96,
      "grad_norm": 0.7211166620254517,
      "learning_rate": 1.1090580263921579e-05,
      "loss": 0.8598,
      "step": 5720
    },
    {
      "epoch": 16.99,
      "grad_norm": 0.7507683634757996,
      "learning_rate": 1.087819623395222e-05,
      "loss": 0.7983,
      "step": 5730
    }
  ],
  "logging_steps": 10,
  "max_steps": 6740,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "total_flos": 8.068310697276211e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
